4. Syntax Analysis

Syntax specified by Context-Free Grammars in EBNF notation.

- From certain classes of grammars, parsers can be automatically created.
- A parser contructs a parse tree if the input is well-formed and passes it along for further processing.
- Parse tree need not be constructed explicitly, i.e. the actions need not construct nodes.

Three types:
- Universal
- Top Down
- Bottom Up

Universal:
Cocke-Younger-Kasami
Earley
* can parse any grammar, but suffer from efficiency problems

Top Down:
Builds parse tree from the top, or root, node to the bottom, or leaf nodes.

Bottom Up:
Starts from the leafs, and works it's way up to the root.

The most efficient top-down and bottom-up parsers work only for a subclass
of grammars. LL and LR in particular.

Expressions are more challenging than constructs that begin with a language keyword to parse,
and require the consideration of precedence and associativity, i.e. PEMDAS.

E: E + T | T    <- lowest precedence, left associativity
T: T * F | F    <- left associativity
F: (E) | id     <- highest precedence

The above grammar can be handled by an LR parser, but not a LL parser, because
the LL parser would loop forever on the left-recursive non-terminals E and T.

By removing left recursion the above grammar becomes suitable for LL parsing:

E: T E'
E': + T E' | ε
T: F T'
T': * F T' | ε
F: (E) | id

For illustrating ambiguity:
E: E + E
 | E * E
 | (E)
 | id

Error handling:
- Panic-Mode
- Phrase-Level Recovery

Common Programming Errors:
Lexical Errors, e.g. misspellings
Syntactic Errors, e.g. misplaced braces
Semantic Errors, e.g. type errors for operands
Logical Errors, e.g. usually incorrect reasoning

Parsers detect errors generally as soon as possible, and they have the
*viable-prefix property*, meaning that they detect an error as soon as they see
a prefix of the input that cannot be completed to form a string of the language.

Generally errors are reported along with context, for example the line of
the offending token(s), and the location of the first error. Reporting of errors
to the user is, in general, a hard problem.

Also, should the error reporter give up as soon as the first error is encountered?
Or should it continue attempting to parse and reporting multiple? Is there a limit
to the number of errors that should be reported?

Recovery Strategies:
Panic Mode
    Bail and report to the user, maybe with context
Phrase Level
    Attempt to recover by replacing the offending token(s), and logging a warning
Error Productions
    Adding productions to the grammar for commonly-occuring syntactical errors, with
    the semantic action of reporting that this error was generated by the parser.
    For example:
        assign_stmt: opttype id '=' expr { assignment }
        type_id_order_error: id opttype '=' expr { report error }
Global Correction

Context Free Grammars

*** The notion of derivations is very helpful for discussing the order in which productions
are applied during parsing. ***

Terminals
Non-Terminals           <- maps to a set of strings
Start Symbol            <- the set of strings the starting non-terminal denotes is the language for the grammar, L(G)
Productions

Example Expression Grammar:

For terminals: id + - * / ( )

expr   ::= expr + term
expr   ::= expr - term
expr   ::= term
term   ::= term * factor
term   ::= term / factor
term   ::= factor
factor ::= (expr)
factor ::= id

Using the | shorthand:

expr   ::= expr + term | expr - term | term
term   ::= term * factor | term / factor | factor
factor ::= (expr) | id

** Derivations **
https://en.wikipedia.org/wiki/Rewriting

The construction of a parse tree can be viewed as a term rewriting system, where each
production is a rewriting rule.

If at each step, beginning with the start symbol, you rewrite a non-terminal,
this corresponds to a top-down construction of the parse tree.

If you rewrite the rightmost non-terminal this corresponds to a "rightmost derivation" used
in bottom-up parsers.

Consider:

E ::= E + E | E * E | - E | ( E ) | id

***
E => - E then denotes rewriting an E term to - E

"E derives -E" or "-E is derived from E"

***

A sequence of replacements, written:
E => - E => - ( E ) => - ( id )
is a **deriviation** of - ( id ) from E

In general:
aAb where a and b are arbitrary strings of grammar symbols

If A ::= γ
Then we can write aAb => aγb
That is, "aAb derives in one step aγb" or "aγb is derived from aAb in one step"

Derivation operators:
=>  "derives in one step"
*=> "derives in zero or more steps"
+=> "derives in one or more steps"

Laws? TODO: identify these
a *=> a for any string a
If a *=> b, and b *=> γ, then a *=> γ           <- is *=> transitive?

Where a is a terminal:
Where S is the start symbol of grammar G:
if S *=> a, then we can say that a is a "sentential form" of G

As an example:

expr ::= expr + expr
expr ::= 4

** A "sentential form" may contain both terminals and non-terminals, i.e.
   it does NOT need to be the final derived string of terminals
   E.g.: expr + expr is a "sentential form" of expr

** A "sentence" of G is a "sentential form" of G with no non-terminals
   E.g.: expr + expr is not a "sentence" of G, but 4 + 4 is a "sentence" of G

** The "language" of a grammar is the set of sentences that can be derived from G

** A language generated by a grammar is a "context-free" language

** If two grammars generate the same language, then the grammars are "equivalent"

For example, this derivation:

E => - E => - ( E ) => - ( E + E ) => - ( id + E ) => - ( id + id )

E, -E, - ( E ), - (E + E), and - (id + E) are all sentential forms of E
- (id + id) is a sentence of E

That is, E *=> - ( id + id )
"E derives - ( id + id ) in zero or more steps"
"- (id + id ) is derived from E in zero or more steps"

This is another possible derivation of - (id + id ):
** Note the choice of non-terminal that gets replaced in the penultimate step **
E => - E => - ( E ) => - ( E + E ) => - ( E + id ) => - ( id + id )

Generally:
At each *step* (i.e. =>) two choices:
Which non-terminal to try replacing (or rewriting),
and which production to try the replacement with

Finally, and so:

*** A "leftmost" derivation is the derivation where, at each step,
    the "leftmost" non-terminal in a sentential form is chosen for replacement ***
*** A "rightmost" derivation is the derivation where, at each step,
    the "rightmost" non-terminal in a sentential form is chosen for replacement ***

This is a leftmost derivation:            ↓
E => - E => - ( E ) => - ( E + E ) => - ( id + E ) => - ( id + id )

This is a rightmost derivation:               ↓
E => - E => - ( E ) => - ( E + E ) => - ( E + id ) => - ( id + id )

So in wAb, w is a string of terminals, A is a non-terminal, and b is a string of terminals and non-terminals
If A ::= γ, then wγb is the next step in a leftmost derivation
Note that in a rightmost derivation, b would be the string of terminals,
and w would be the string of non-terminals and terminals that still need replacing

TODO: Why are rightmost derivations called _canonical_ derivations?

Parse Trees

Each interior node represents the application of a production. For both of the above derivations:
- ( id + id ) has the parse tree:

Derivations of - (id + id) from E

Leftmost:                                 ↓
E => - E => - ( E ) => - ( E + E ) => - ( id + E ) => - ( id + id )

Rightmost:                                    ↓
E => - E => - ( E ) => - ( E + E ) => - ( E + id ) => - ( id + id )

Grouped (as shown in the parse tree):
1.    E
2. => - E
3. => - ( E )
4. => - ( E + E )
5. => - ( id + id )

Parse Tree for - (id + id) from E
"E derives - (id + id) in 0 or more steps"
E *=> - (id + id)


  +-------  E  ---------------+                       Step 1: E
  |                           |
  |                           |
  -            +------------  E  ------------+        Step 2: - E
               |              |              |
               |              |              |
               (      +-----  E  -----+      )        Step 3: - ( E )
                      |       |       |
                      |       |       |
                      E       +       E               Step 4: - ( E + E )
                      |               |
                      |               |
                     id              id               Step 5: - ( id + id ) (the yield)

** Because parse trees ignore the variations in the order in which symbols in sentential forms are
   replaced, there is a MANY to ONE relationship between derivations and parse trees **

** Every parse tree has a unique leftmost and a unique rightmost derivation **
** If a grammar produces more than one parse tree, it also produces more than one
   leftmost and rightmost derivation, and is said to be ambiguous **

Ambiguity:

A grammar that produces more than one parse tree for some sentence is said to be "ambiguous".
i.e. it produces more than one leftmost derivation or rightmost derivation for the sentence.

E ::= E + E
    | E * E
    | - E
    | ( E )
    | id

For sentence id + id * id, there are two distinct leftmost derivations E *=> id + id * id:

The difference between the two being whether we construct the addition sentential form (E + E) first
or the multiplication (E * E) sentential form

id + id * id derived from E:

   E
=> E + E
=> id + E
=> id + E * E
=> id + id * E
=> id + id * id

   E
=> E * E
=> E + E * E
=> id + E * E
=> id + id * E
=> id + id * id

Context Free Grammars versus Regular Expressions

Grammars are more powerful as a notation
I.e. Every construct that can be described by a regular expression can be described by a
     context free grammar but not vice-versa

regular expression (a|b)*abb and grammar:
A ::= aA | bA | aB
B ::= bC
C ::= bD
D ::= ε

both describe the same language

OTOH
L = { a^nb^n | n >= 1 }
is an example of a language that a regular expression cannot describe

"finite automata cannot count"
"a grammar can count two items, but not three"

S ::= S S + | S S * | a
aa+a*
S *=> aa+a*
Leftmost Derivation:
    S
=>  S S *
=>  S S + S *
=>  a S + S *
=>  a a + S *
=>  a a + a *

Rightmost Derivation:
    S
=>  S S *
=>  S a *
=>  S S + a *
=>  S a + a *
=>  a a + a *

Writing a Grammar
Grammars can describe most of the syntax of programming languages, but can't capture notions like
"ids must be declared before they can be used within a given block".
A semantically invalid program may still be syntactically valid

Not all grammars are suitable for parsing with a computer, but usually we can perform some
transformations on the grammar to get it into a state that makes it suitable for parsing.

Eliminating Ambiguity:
stmt ::= if expr then stmt
       | if expr then stmt else stmt
       | other

if E1 then if E2 then S1 else S2

stmt *=> if E1 then if E2 then S1 else S2
Because a unique leftmost derivation corresponds to one parse tree, we then have two possible
parse trees for the above grammar.

Derivation #1:
    stmt
=>  if expr then stmt                           <- multiple productions work here
=>  if expr then if expr then stmt else stmt    <- sentential form
=>  if E1 then if E2 then S1 else S2

Derivation #2:
    stmt
=>  if expr then stmt else stmt                 <- multiple productions work here
=>  if expr then if expr then stmt else stmt
=>  if E1 then if E2 then S1 else S2

Derivation #1 is preferred. We try to keep "then" and "else" grouped at each node.

stmt ::= matched_stmt
       | open_stmt
matched_stmt ::= if expr then matched_stmt else matched_stmt
               | other
open_stmt ::= if expr then stmt
            | if expr then matched_stmt else open_stmt

TODO: find a way to systematically remove ambiguity from a grammar

A grammar is "left recursive" if there is a derivation A +=> Aa for some string a

Removing direct left recursion:
A ::= Aa
    | b

A ::= bA'               <- head production
A' ::= aA' | ε          <- tail productions

An indirectly left recursive grammar:
S ::= A a | b
A ::= A c | S d | ε
S => A a => S d a, so S +=> S d a, so S is indirectly left recursive

This can be systematically removed as long as the grammar contains no cycles and no ε productions:
I.e. A +=> A, and A ::= ε

See Algorithm 4.19

Let G be:
S ::= A a | b
A ::= A c | S d | ε

Arrange the non-terminals of the grammar G in some order: A₁,A₂,...,An
For i in [1, n]:
    For j in [1, i - 1]:
        Replace each production of the form Aᵢ ::= Aⱼγ by the productions
        Aᵢ ::= δ₁γ,δ₂γ,...,δkγ where
        Aⱼ ::= δ₁,δ₂,...,δk are all current Aⱼ productions.
        I.e. copy the productions from Aⱼ into Aᵢ, but with the trailing symbols γ
             of the production being replaced.
    Eliminate direct left recursion from the Aᵢ productions

A > S

S ::= A a | b
A ::= A c | S d | ε

becomes

S ::= A a | b
A ::= A c | A a d | b d | ε     <- replace the S d rule with the productions from S trailing d

becomes

S ::= A a | b
A ::= b d A' | A'               <- epsilon gets replaced with tail
A' ::= c A' | a d A' | ε        <- new tail productions after removing left recursion

And done!

Left Factoring

stmt ::= if expr then stmt
    | if expr then stmt else stmt
    | other

factor out "if expr then stmt":

stmt ::= if expr then stmt stmt' | other
stmt' ::= else stmt | ε

Top Down Parsing
- Creating nodes for a parse tree starting from the root and in depth-first preorder
- Equivalently finding a leftmost derivation for an input string
