4. Syntax Analysis

Syntax specified by Context-Free Grammars in EBNF notation.

- From certain classes of grammars, parsers can be automatically created.
- A parser contructs a parse tree if the input is well-formed and passes it along for further processing.
- Parse tree need not be constructed explicitly, i.e. the actions need not construct nodes.

Three types:
- Universal
- Top Down
- Bottom Up

Universal:
Cocke-Younger-Kasami
Earley
* can parse any grammar, but suffer from efficiency problems

Top Down:
Builds parse tree from the top, or root, node to the bottom, or leaf nodes.

Bottom Up:
Starts from the leafs, and works it's way up to the root.

The most efficient top-down and bottom-up parsers work only for a subclass
of grammars. LL and LR in particular.

Expressions are more challenging than constructs that begin with a language keyword to parse,
and require the consideration of precedence and associativity, i.e. PEMDAS.

E: E + T | T    <- lowest precedence, left associativity
T: T * F | F    <- left associativity
F: (E) | id     <- highest precedence

The above grammar can be handled by an LR parser, but not a LL parser, because
the LL parser would loop forever on the left-recursive non-terminals E and T.

By removing left recursion the above grammar becomes suitable for LL parsing:

E: T E'
E': + T E' | ε
T: F T'
T': * F T' | ε
F: (E) | id

For illustrating ambiguity:
E: E + E
 | E * E
 | (E)
 | id

Error handling:
- Panic-Mode
- Phrase-Level Recovery

Common Programming Errors:
Lexical Errors, e.g. misspellings
Syntactic Errors, e.g. misplaced braces
Semantic Errors, e.g. type errors for operands
Logical Errors, e.g. usually incorrect reasoning

Parsers detect errors generally as soon as possible, and they have the
*viable-prefix property*, meaning that they detect an error as soon as they see
a prefix of the input that cannot be completed to form a string of the language.

Generally errors are reported along with context, for example the line of
the offending token(s), and the location of the first error. Reporting of errors
to the user is, in general, a hard problem.

Also, should the error reporter give up as soon as the first error is encountered?
Or should it continue attempting to parse and reporting multiple? Is there a limit
to the number of errors that should be reported?

Recovery Strategies:
Panic Mode
    Bail and report to the user, maybe with context
Phrase Level
    Attempt to recover by replacing the offending token(s), and logging a warning
Error Productions
    Adding productions to the grammar for commonly-occuring syntactical errors, with
    the semantic action of reporting that this error was generated by the parser.
    For example:
        assign_stmt: opttype id '=' expr { assignment }
        type_id_order_error: id opttype '=' expr { report error }
Global Correction

Context Free Grammars

*** The notion of derivations is very helpful for discussing the order in which productions
are applied during parsing. ***

Terminals
Non-Terminals           <- maps to a set of strings
Start Symbol            <- the set of strings the starting non-terminal denotes is the language for the grammar, L(G)
Productions

Example Expression Grammar:

For terminals: id + - * / ( )

expr   ::= expr + term
expr   ::= expr - term
expr   ::= term
term   ::= term * factor
term   ::= term / factor
term   ::= factor
factor ::= (expr)
factor ::= id

Using the | shorthand:

expr   ::= expr + term | expr - term | term
term   ::= term * factor | term / factor | factor
factor ::= (expr) | id

** Derivations **
https://en.wikipedia.org/wiki/Rewriting

The construction of a parse tree can be viewed as a term rewriting system, where each
production is a rewriting rule.

If at each step, beginning with the start symbol, you rewrite a non-terminal,
this corresponds to a top-down construction of the parse tree.

If you rewrite the rightmost non-terminal this corresponds to a "rightmost derivation" used
in bottom-up parsers.

Consider:

E ::= E + E | E * E | - E | ( E ) | id

***
E => - E then denotes rewriting an E term to - E

"E derives -E" or "-E is derived from E"

***

A sequence of replacements, written:
E => - E => - ( E ) => - ( id )
is a **deriviation** of - ( id ) from E

In general:
aAb where a and b are arbitrary strings of grammar symbols

If A ::= γ
Then we can write aAb => aγb
That is, "aAb derives in one step aγb" or "aγb is derived from aAb in one step"

Derivation operators:
=>  "derives in one step"
*=> "derives in zero or more steps"
+=> "derives in one or more steps"

Laws? TODO: identify these
a *=> a for any string a
If a *=> b, and b *=> γ, then a *=> γ           <- is *=> transitive?

Where a is a terminal:
Where S is the start symbol of grammar G:
if S *=> a, then we can say that a is a "sentential form" of G

As an example:

expr ::= expr + expr
expr ::= 4

** A "sentential form" may contain both terminals and non-terminals, i.e.
   it does NOT need to be the final derived string of terminals
   E.g.: expr + expr is a "sentential form" of expr

** A "sentence" of G is a "sentential form" of G with no non-terminals
   E.g.: expr + expr is not a "sentence" of G, but 4 + 4 is a "sentence" of G

** The "language" of a grammar is the set of sentences that can be derived from G

** A language generated by a grammar is a "context-free" language

** If two grammars generate the same language, then the grammars are "equivalent"

For example, this derivation:

E => - E => - ( E ) => - ( E + E ) => - ( id + E ) => - ( id + id )

E, -E, - ( E ), - (E + E), and - (id + E) are all sentential forms of E
- (id + id) is a sentence of E

That is, E *=> - ( id + id )
"E derives - ( id + id ) in zero or more steps"
"- (id + id ) is derived from E in zero or more steps"

This is another possible derivation of - (id + id ):
** Note the choice of non-terminal that gets replaced in the penultimate step **
E => - E => - ( E ) => - ( E + E ) => - ( E + id ) => - ( id + id )

Generally:
At each *step* (i.e. =>) two choices:
Which non-terminal to try replacing (or rewriting),
and which production to try the replacement with

Finally, and so:

*** A "leftmost" derivation is the derivation where, at each step,
    the "leftmost" non-terminal in a sentential form is chosen for replacement ***
*** A "rightmost" derivation is the derivation where, at each step,
    the "rightmost" non-terminal in a sentential form is chosen for replacement ***

This is a leftmost derivation:            ↓
E => - E => - ( E ) => - ( E + E ) => - ( id + E ) => - ( id + id )

This is a rightmost derivation:               ↓
E => - E => - ( E ) => - ( E + E ) => - ( E + id ) => - ( id + id )

So in wAb, w is a string of terminals, A is a non-terminal, and b is a string of terminals and non-terminals
If A ::= γ, then wγb is the next step in a leftmost derivation
Note that in a rightmost derivation, b would be the string of terminals,
and w would be the string of non-terminals and terminals that still need replacing

TODO: Why are rightmost derivations called _canonical_ derivations?

Parse Trees

Each interior node represents the application of a production. For both of the above derivations:
- ( id + id ) has the parse tree:

Derivations of - (id + id) from E

Leftmost:                                 ↓
E => - E => - ( E ) => - ( E + E ) => - ( id + E ) => - ( id + id )

Rightmost:                                    ↓
E => - E => - ( E ) => - ( E + E ) => - ( E + id ) => - ( id + id )

Grouped (as shown in the parse tree):
1.    E
2. => - E
3. => - ( E )
4. => - ( E + E )
5. => - ( id + id )

Parse Tree for - (id + id) from E
"E derives - (id + id) in 0 or more steps"
E *=> - (id + id)


  +-------  E  ---------------+                       Step 1: E
  |                           |
  |                           |
  -            +------------  E  ------------+        Step 2: - E
               |              |              |
               |              |              |
               (      +-----  E  -----+      )        Step 3: - ( E )
                      |       |       |
                      |       |       |
                      E       +       E               Step 4: - ( E + E )
                      |               |
                      |               |
                     id              id               Step 5: - ( id + id ) (the yield)

** Because parse trees ignore the variations in the order in which symbols in sentential forms are
   replaced, there is a MANY to ONE relationship between derivations and parse trees **

** Every parse tree has a unique leftmost and a unique rightmost derivation **
** If a grammar produces more than one parse tree, it also produces more than one
   leftmost and rightmost derivation, and is said to be ambiguous **

Ambiguity:

A grammar that produces more than one parse tree for some sentence is said to be "ambiguous".
i.e. it produces more than one leftmost derivation or rightmost derivation for the sentence.

E ::= E + E
    | E * E
    | - E
    | ( E )
    | id

For sentence id + id * id, there are two distinct leftmost derivations E *=> id + id * id:

The difference between the two being whether we construct the addition sentential form (E + E) first
or the multiplication (E * E) sentential form

id + id * id derived from E:

   E
=> E + E
=> id + E
=> id + E * E
=> id + id * E
=> id + id * id

   E
=> E * E
=> E + E * E
=> id + E * E
=> id + id * E
=> id + id * id

Context Free Grammars versus Regular Expressions

Grammars are more powerful as a notation
I.e. Every construct that can be described by a regular expression can be described by a
     context free grammar but not vice-versa

regular expression (a|b)*abb and grammar:
A ::= aA | bA | aB
B ::= bC
C ::= bD
D ::= ε

both describe the same language

OTOH
L = { a^nb^n | n >= 1 }
is an example of a language that a regular expression cannot describe

"finite automata cannot count"
"a grammar can count two items, but not three"

S ::= S S + | S S * | a
aa+a*
S *=> aa+a*
Leftmost Derivation:
    S
=>  S S *
=>  S S + S *
=>  a S + S *
=>  a a + S *
=>  a a + a *

Rightmost Derivation:
    S
=>  S S *
=>  S a *
=>  S S + a *
=>  S a + a *
=>  a a + a *

Writing a Grammar
Grammars can describe most of the syntax of programming languages, but can't capture notions like
"ids must be declared before they can be used within a given block".
A semantically invalid program may still be syntactically valid

Not all grammars are suitable for parsing with a computer, but usually we can perform some
transformations on the grammar to get it into a state that makes it suitable for parsing.

Eliminating Ambiguity:
stmt ::= if expr then stmt
       | if expr then stmt else stmt
       | other

if E1 then if E2 then S1 else S2

stmt *=> if E1 then if E2 then S1 else S2
Because a unique leftmost derivation corresponds to one parse tree, we then have two possible
parse trees for the above grammar.

Derivation #1:
    stmt
=>  if expr then stmt                           <- multiple productions work here
=>  if expr then if expr then stmt else stmt    <- sentential form
=>  if E1 then if E2 then S1 else S2

Derivation #2:
    stmt
=>  if expr then stmt else stmt                 <- multiple productions work here
=>  if expr then if expr then stmt else stmt
=>  if E1 then if E2 then S1 else S2

Derivation #1 is preferred. We try to keep "then" and "else" grouped at each node.

stmt ::= matched_stmt
       | open_stmt
matched_stmt ::= if expr then matched_stmt else matched_stmt
               | other
open_stmt ::= if expr then stmt
            | if expr then matched_stmt else open_stmt

TODO: find a way to systematically remove ambiguity from a grammar

A grammar is "left recursive" if there is a derivation A +=> Aa for some string a

Removing direct left recursion:
A ::= Aa
    | b

A ::= bA'               <- head production
A' ::= aA' | ε          <- tail productions

An indirectly left recursive grammar:
S ::= A a | b
A ::= A c | S d | ε
S => A a => S d a, so S +=> S d a, so S is indirectly left recursive

This can be systematically removed as long as the grammar contains no cycles and no ε productions:
I.e. A +=> A, and A ::= ε

See Algorithm 4.19

Let G be:
S ::= A a | b
A ::= A c | S d | ε

Arrange the non-terminals of the grammar G in some order: A₁,A₂,...,An
For i in [1, n]:
    For j in [1, i - 1]:
        Replace each production of the form Aᵢ ::= Aⱼγ by the productions
        Aᵢ ::= δ₁γ,δ₂γ,...,δkγ where
        Aⱼ ::= δ₁,δ₂,...,δk are all current Aⱼ productions.
        I.e. copy the productions from Aⱼ into Aᵢ, but with the trailing symbols γ
             of the production being replaced.
    Eliminate direct left recursion from the Aᵢ productions

A > S

S ::= A a | b
A ::= A c | S d | ε

becomes

S ::= A a | b
A ::= A c | A a d | b d | ε     <- replace the S d rule with the productions from S trailing d

becomes

S ::= A a | b
A ::= b d A' | A'               <- epsilon gets replaced with tail
A' ::= c A' | a d A' | ε        <- new tail productions after removing left recursion

And done!

Left Factoring

stmt ::= if expr then stmt
    | if expr then stmt else stmt
    | other

factor out "if expr then stmt":

stmt ::= if expr then stmt stmt' | other
stmt' ::= else stmt | ε

Top Down Parsing
- Creating nodes for a parse tree starting from the root and in depth-first preorder
- Equivalently finding a leftmost derivation for an input string

The class of grammars for which we can construct predictive parsers is called the LL(k) class.

FIRST
FOLLOW

From the first and follow sets of a grammar we can construct "predictive parse tables".

Recursive descent parser consists of a set of procedures corresponding to non-terminals

For grammar G we can compute these two functions:
FIRST(G)
FOLLOW(G)

first(α)
    where α is a string of grammar symbols to be the set of terminals
    that begin strings derived from α
    "if alpha can derive epsilon in zero or more steps..."
    if α *=> ε, then ε is also in first(α)

first(α) example
stmt ::= if expr then stmt
       | while expr do stmt
       | do stmt while expr
       | type id = expr

first(stmt) = { if, while, do, type, }

follow(A)
    for nonterminal A to be the set of terminals that can appear
    immediately to the right of A in some sentential form
    i.e., the set of terminals b such that there exists a derivation
    S *=> aAbB
    if A can be the rightmost symbol in some sentential form the special $
    marker is used

first(X) algorithm:
if X is a terminal, then first(X) is { X }
if X is a nonterminal, and X ::= AB...C is a production of length >= 1
    then place a in first(X) if for some i, a is in
    TODO: figure this out

For grammar:

expr:       term expr_tail
expr_tail:  + term expr_tail | ε
term:       factor term_tail
term_tail:  * factor term_tail | ε
factor:     ( expr ) | id

first(factor) = { (, id }
first(term) = first(factor)
first(expr) = first(term)

So first(expr) = first(term) = first(factor) = { (, id }
first(expr_tail) = { +, ε }
first(term_tail) = { *, ε }

follow(expr) = follow(expr_tail) = { ), $ }
follow(term) = follow(term_tail) = { +, ), $ }
follow(factor) = { +, *, ), $ }

So first(A) is the set of terminals that can prefix sentential forms derived from A
and follow(A) is the set of terminals that can follow A in sentential forms derived from A
In the above, +, *, ), and $ can all follow a factor

So for LL(1):
The first "L" stands for scanning the input from left to right
The second "L" stands for taking the leftmost derivation for each step,
and (1) indicates that we'll only need one token of input to make the decision on which derivation to choose

Transition Diagrams can be constructed for predictive parsers

A Predictive Parsing table can be constructed using first and follow sets
M[A, a] where A is a non-terminal, and a is a terminal or the end of input marker $

For each production A ::= α in the grammar:
    For each terminal t in first(α) add M[A, t] = α
    If ε is in first(α), then for each terminal s in follow(A) add M[A, s] = α
    If ε is in first(α), and $ is in follow(A) add M[A,$] = α
    If there is then no production in M[A, t], then set M[A, t] to error (empty entry)

See Figure 4.17 on page 225

Nonrecursive Predictive Parsing
Maintain an explicit stack

E: T E'
E': + T E' | ε
T: F T'
T': * F T' | ε
F: ( E ) | id

enum symbols {
    EOF,
    EXPR,
    EXPR_TAIL,
    TERM,
    TERM_TAIL,
    FACTOR,
    ID,
};

E +=> id + id * id

      E
lm=>  T E'
lm=>  F T' E'
lm=>  id T' E'
lm=>  id ε E'
lm=>  id ε + T E'
lm=>  id ε + F T' E'
lm=>  id ε + id T' E'
lm=>  id ε + id * F T' E'
lm=>  id ε + id * id T' E'
lm=>  id ε + id * id ε ε

// stack-based LL(1) predictive parser
push(stack, EOF)
push(stack, EXPR);

tok = scan(input)
while (!empty(stack) && (sym = pop(stack))) {
    switch (sym) {
        case EOF:
            return true
            break;
        case EXPR:
            push(stack, EXPR_TAIL);
            push(stack, TERM);
            break;
        case EXPR_TAIL:
            if (tok == '+')
                push(stack, EXPR_TAIL);
                push(stack, TERM);
                push(stack, '+');
            break;
        case TERM:
            push(stack, TERM_TAIL);
            push(stack, FACTOR);
            break;
        case TERM_TAIL:
            if (tok == '*')
                push(stack, TERM_TAIL);
                push(stack, FACTOR);
                push(stack, '*');
            break;
        case FACTOR:
            if (tok == '(')
                push(stack, ')');
                push(stack, EXPR);
                push(stack, '(');
            else
                push(stack, ID);
            break;
        default:
            if (tok != sym) {
                return false
            }
            break;
    }

    tok = scan(input);
}


Bottom Up Parsing

Constructs a parse tree by working from the leaves (the bottom) up to the root.
Considered the process of "reducing" a start string to the start symbol of a grammar.

- Each "reduction" step a specific substring matching the body of a production is replaced by
  that productions head.

id * id     <- start string
F * id      <- reduce the first id to F, because F => id
T * id      <- reduce the F to T, because T => F
T * F       <- now we have a choice, either reduce T to E, because E => T, or the second id to F, because F => id
T           <- after choosing the latter, we reduce T * F to T, because T => T * F
E           <- finally, T reduces to E, because E => T

** A reduction is the inverse of a step in a derivation, i.e. the inverse of the rm=> operator **

Bottom up parsing during a left to right scan of the input constructs a rightmost derivation in reverse

The process involves finding a "reducing production" for the rightmost handle in a sentential form

Shift-Reduce

Stack       Input           Action
$           id * id $       Shift
$ id        * id $          Reduce by F => id
$ F         * id $          Reduce by T => F
$ T         * id $          Shift               <- what prevents us from using E => T here?
$ T *       id $            Shift
$ T * id    $               Reduce by F => id
$ T * F     $               Reduce by T => T * F
$ T         $               Reduce by T => T * F
$ T         $               Reduce by E => T
$ E         $               Accept

Shift, Reduce, Accept, or Error

LR Parsing
Simple LR Parsing

Items
Parser States

More Complex Methods:
LALR
Canonical LR

LR Parsers are table-driven

- Can recognize virtually all programming language constructs for which a CFG can be written
- The most general shift-reduce parser known
- Can detect syntax errors right away
- Can parse a superset of the grammars that a LL parser can parse

*** How does a shift reduce parser know when to shift and when to reduce? ** See above question
Answer: It keeps track of states to know where in the parse we are
"States" represent sets of "items"
An item is a production of a grammar with a dot (marker) at some position in the body
A: . X Y Z | ε
A: X . Y Z | ε
A: X Y . Z | ε
A: X Y Z . | ε
A: X Y Z | ε .

So production A has 5 items
It indicates how much of a production we have seen during the parsing process.

One collection of sets of LR(0) items, called the "canonical" LR(0) colleciton lets us construct
a deterministic finite automaton to make aprsing decisions, the LR(0) automaton

To construct it we need an augmented grammar, and the functions CLOSURE and GOTO
Grammar G
S: ...

(Augmented) Grammar G'
S': S       <- Stop parsing and accept, i.e. when the parser is about to reduce to S'

I is a set of items for Grammar G
CLOSURE(I) is the set of items constructed from I by two rules:
    Add items from I to CLOSURE(I)
    If A: a . B b is in CLOSURE(I) and B: C is a production, then add the item B: . C
    to CLOSURE(I) if it's not already there. Apply this rule until no more new items are added to CLOSURE(I)

The augmented grammar:
E': E
E: E + T | T
T: T * F | F
F: ( E ) | id

E.g.:
I is { E': . E }
CLOSURE(I) is
{
    E': . E,
     E: . E + T,
     E: . T,
     T: . T * F,
     T: . F,
     F: . ( E ),
     F: . id
}

struct set closure(struct set items) {
    struct set clo_items = items
    repeat
        for (item A: a . B b in clo_items)
            for (production B: c in grammar G)
                if (item B: . c is not in clo_items)
                    add item B: . c to clo_items
    until no more items are added to j
    return clo_items
}

Partition items of interest into two groups:
Kernel Items: The initial item, S': . S, and all items whose dots are not at the left end
Nonkernel items: all items with their dots at the left end, except for S': . S

Items added to the closure cannot be Kernel Items

Then...

GOTO(I, X) where
    I is a set of closure items
    X is a grammar symbol

    The closure of the set of all items A: a X . b such that
    A a . X b is in I
    I.e. to define the transitions in the automaton for the grammar

E': E
E: E + T | T
T: T * F | F
F: ( E ) | id

E.g. If I is the set:
{
    E': E .,
    E: E . + T
}
Then GOTO(I, +) is the set:
{
    E: E + . T,
    T: . T * F,
    T: . F,
    F: . ( E ),
    F: . id,
}

So, C, the canonical collection LR(0) is:
void items(grammar G') {
    set of sets C = { CLOSURE(item S': . S) }
    repeat
        for (set I in C)
            for (grammar symbol X)
                if GOTO(I, X) is not empty and not in C
                    add GOTO(I, X) to C
    until no new sets are added to C
}

The start state of the LR(0) automaton is CLOSURE(item S': . S)

*** LR Shift-Reduce Decision

So whether to shift or to reduce is done like:
string w of grammar symbols takes the LR(0) automata from start state 0 to some state j
Then ***shift*** on the next input symbol a if state j has a transition on a,
Otherwise ***reduce***. The items in state J tell us which production to reduce with.
