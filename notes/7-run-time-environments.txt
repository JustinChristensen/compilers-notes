7. Run Time Environments

Storage

Typically run-time memory is partioned into two main segments: Code and Data

A byte is the smallest addressable unit of memory.

Addressing constraints of the target machine may require that the storage
layout of data objects be *aligned*, i.e. placed at an address divisible by 4

A compiler may allocate more storage then is needed to ensure proper alignment.

For instance, allocating an array of 10 characters may only require 10 bytes,
but a compiler may choose to allocate 12 bytes to align the data on 4 byte boundaries.

Unused storage allocated in this way is called *padding*.

Where space is at a premium, a compiler may instead choose to *pack* the data
so that none of the allocated space goes unused, but this will likely come at the cost
of additional instructions to be executed at runtime to ensure the packed data is properly
positioned for operation.

Size of target code is fixed at compile time, and is placed in a
statically determined area, *Code*.

Where the size of data objects is known at compile time, some global objects
may be placed in a statically determined area called *Static*.

*Stack* and *Heap* are positioned at the end of the remaining address space for the process.

So it's:

Code < Static < Heap -> [ Free Memory ] <- Stack

(Of course this implies that the Heap is already part of the processes' allocated address space)

Stack stores *activation records* that get generated during procedure calls
In practice the stack grows downward, that is towards LOWER addresses
The heap grows towards higher addresses

When handling procedure calls there's this idea of a "control". Control
can be given and returned to procedures, and we store things like the program
counter and the values of registers just before shifting control to a called procedure.
Then, when control is returned we can restore the state of the registers in the calling procedure.

Programming languages allow the programmer to allocate and deallocate memory
from the heap.

7.1.1 Static Versus Dynamic Storage Allocation

Layout and allocation of data to memory locations in the RTE are key issues in storage management.

"Static" and "Dynamic" distinguish between compile-time and run-time, respectively.

A storage allocation decision is *static* if it can be made by the compiler looking only at the source
of the program, not at what the program does during execution.

A storage allocation decision is *dynamic* only if it can be made while the program is running.

Dynamic storage allocation usually falls two either of:
1. Stack storage (aka. the run-time stack) or
2. Heap storage

The heap is an area of virtual memory

7.2 Stack Allocation

Activations (stack frames) of procedures nest in time.

We can represent the activations of procedures with a tree.
Node corresponds to an activation, where the root node is the main procedure.
Child nodes correspond to activations of the procedures called by this activation.
A child must finish before it's sibling can begin.

The sequence of procedure calls corresponds to a PREORDER traversal of the activation tree
The sequence of returns corresponds to a POSTORDER traversal of the tree
Activations that are LIVE for a given node N are itself, and it's ancestors

Procedure calls and returns are managed by a run-time stack called the *control stack*

The *control stack* has an *activation record* (stack frame) for each live activation.

Activation records (stack frames) may contain all or any of the following:
- Parameters
- Returned Values
- Control Link
- Access Link
- Saved Machine Status
- Local Data
- Temporaries

* Temporary values for intermediate values in expressions when register space is limited
* Saved Machine Status typically includes the *return address* (value of the program counter to which the procedure must return),
    and the contents of registers that were in use by the calling procedure and must be restored
* Access links point to data needed by the current procedure that may be stored elsewhere
* Control links point to the activation record of the calling procedure
* Potentially space for the return value, if register storage is not preferred for some reason
* Parameters if we don't have enough registers for them

7.2.3 Calling Sequences

Procedure calls are implemented by what are known as *calling sequences*.
    - allocates an activation record on the stack, and populates it

A *return sequence* restores the state of the machine, so the calling sequence can continue

Calling sequences and activation records (stack frame) differ from language to language and compiler to compiler.

Design principles for calling sequences/stack frames:
    - Values transferred from the caller to the callee are placed at the beginning of the stack frame.
        The caller can compute the values without having to create the entire next allocation record,
        and it allows for procedures that take variadic arguments like printf
    - Fixed-length items are generally placed in the middle. I.e. the control link, access link, and
        machine status fields.
    - While local variables typically have a fixed size according to their types, some locals may be dynamically
        sized (like C's VLAs), and the number of temporaries needed may depend on how code generation allocates registers.
        So local variables typically go at the end of the frame.
    - Generally the top of the stack pointer points to the top of the fixed length fields, just before local data and temporaries
        which again, may be of a variable size.

The calling sequence generally goes as follows:
1. Caller evaluates the parameters
2. Caller stores a return address and the old value of _top_ into the callee's activation
    record. _top_ is then advanced past the caller's local data and the callee's parameters and status fields
3. Callee saves the register values and status information
4. Callee initializes local data and starts executing

An example corresponding return sequence:
1. Callee places the return value next to the parameters
2. Callee restores _top_ back to where it was, and restores machine registers.
    Then, it jumps to the return address provided by the caller.
3. The caller is now free to make use of the return value

Run-time memory management must deal with allocating space for objects of sizes not known at compile time.
That is, objects of a dynamic size.

Variable length data can be allocated based on the value of one or of the accompanying parameters from the caller.

7.3 Access to Nonlocal Data on the Stack

How do procedures access data?
What mechanism is used for finding data for a procedure that doesn't belong to that procedure,
    or when nested declarations are allowed, how to access data from enclosing procedures.

7.3.1 Without Nested Procedures

Languages like C, without nested procedures, have two main scopes: global and local.
    - Globals are allocated as static storage, and the locations are fixed and known at compile time
    - Other names are local to the stack frame, and are accessed through the _top_ pointer

7.3.2 With Nested Procedures

First off, knowing that a procedure's declaration is nested inside of another procedure tells us nothing
about the relative position of the activation records for the procedures at runtime.

7.3.3 Languages w/ Nested Procedures

ALGOL 60 had nested procedures.

See: https://en.wikipedia.org/wiki/Nested_function
See: https://en.wikipedia.org/wiki/Funarg_problem

7.3.5 Access Links

If p is nested in q, then the access link in a stack frame will point to the most
    recent activation of q on the stack
Access links then form a chain from stack frames at the top of the stack to lower stack frames
    - Along this chain are the activations of procedures whose data are accessible by the currently
        live stack frame
    - So a search for the value of x within the current procedure requires a traversal of the access links
        from the top of the stack downward through the access links

So if procedure A contains procedure B and C, and procedure C contains procedure D, which calls procedure B,
    B's activation record will point to the most recent stack frame for procedure A, even though it was called
    from procedure D

The more difficult case comes in with higher order functions, where a procedure parameter may vary

When a procedure A calls another procedure B as a parameter, it won't know how to set the access link,
    because it doesn't know where B is declared.
To solve this, the caller needs to pass the proper access link along with the parameter.

As an example:
fun a(x) =
    let fun b(f) = ... f(...) ...
        fun c(y) =
            let fun d(z) = ...
            in ... b(d) ...
            end
    in
        ... c(1) ...
    end;

Calling a:

1. Push a's activation record on the stack
2. Push c's activation record on the stack, w/ access link pointing to previous stack frame for a
4. When c calls b, it passes the access link pointing to it's activation record along with d on the call.
3. Push b's activation record on the stack, w/ access link pointing to a's recent activation record
5. When d is called through a parameter from b, it sets to access link pointing to c that was provided along with the parameter

So the parameter f is actually a 2-tuple with a pointer to d, and the access link

7.3.8 Displays

Another solution for keeping track of where a nested procedure should look for data is to use *displays*

The display array stores pointers to the highest activation record at each nesting depth level.
So if s calls q which calls q, and q has nesting depth 2, display[2] will point to the latter q, and the activation
record for the previous q call will be stored in that activation record

7.4 Heap Management

Used for data that lives indefinitely or until it's explicitly deleted.
Languages like C++ and Java provide new, which lets the programmer create objects that live longer than the frame that creates them.

A *memory manager* allocates and deallocates space within the heap, and it's an interface
    between applications and the operating system.

Languages like C and C++ are said to allocate and deallocate *manually* by operations like free and delete.

*Garbage collection* is the process of finding spaces within the heap that are no longer being used

Memory Manager performs:

Allocation
    - if no chunk is vailable, it requests virtual memory from the operating system
    - if space is exhausted, it passes that information along to the program
Deallocation
    - returns space to the pool of free space for reuse, but that memory is typically not
      returned to the operating system, even if the heap usage drops

Lisp makes all memory chunks the same size by using a two-pointer cell, from which all structures are built

A memory manager must be prepared to service in any order allocations and deallcations of a variety of sizes,
    and it must be prepared to service requests for one byte all the way to the program's entire address space (?)

Properties:

Space Efficiency
- a memory manager should minimize the total heap space needed by the program (potentially by reducing *fragmentation*)

Program Efficiency
- it should make use of the memory subsystem to make programs run faster
- the time taken to execute an instruction can depend on where the object is located in memory

Low Overhead
- the operations that implement allocation and deallocation must be efficient, because they'll be executed often
- the fraction of time spent executing the allocation and deallocation routines must be kept to a minimum
    TODO: profile time spent in allocation routines in the memory manager in a couple of programming languages
- as the size of allocations decreases, the more relative time the program will spend within the allocation routines

See: http://gee.cs.oswego.edu/dl/html/malloc.html
See: https://en.wikipedia.org/wiki/C_dynamic_memory_allocation
See: https://sourceware.org/glibc/wiki/MallocInternals
See: https://stackoverflow.com/questions/8866790/do-shared-libraries-use-the-same-heap-as-the-application

7.4.2 Memory Hierarchy of a Computer

The time taken to execute an instruction can vary significantly, because the time taken
to access various parts of memory can vary from nanoseconds to milliseconds.

Data-intensive programs can benefit significantly from optimizations that make good use of the memory subsystem,
    (and they can also make use of locality, which is the property of nonrandom access to memory)

Peter Norvig says it takes 100 nanoseconds (1/10 of a microsecond, and 1/10000 of a millsecond) to fetch a word from memory.

** The current limitations on hardware mandate that we can only build small and fast storage, or large and slow storage. **

At the time of the writing of the book it was impossible to build gigabyte storage with nanosecond access times

Almost all computers arrange their storage in a *memory hierarchy*, which is a series of storage elements
    with smaller and faster elements closer to the processor, and larger and slower elements further away.
- A processor has a number of registers, whose contents are under software control
- Then, it has one or more levels of cache, usually made out of SRAM (at the time of the writing), that range from kilobytes to megabytes of size
- Then, it has a main memory, size typically measured in gigabytes of DRAM
- The physical memory is then backed up by virtual memory, implemented as gigabytes of disk space.
- When memory is accessed, the processor looks at each step of the hierarchy to locate it, starting from the registers and moving upwards

Registers are scarce, so the code that the compiler generates manages their use, but higher levels of the hierarchy are managed automatically.
This way, a program can work in a variety of memory hierarchies.

Virtual memory is managed by the operating system, with the assistance of hardware structures like a *translation lookaside buffer*

See: https://en.wikipedia.org/wiki/Translation_lookaside_buffer

An example hierarchy:

    Level           Access Time
Virtual Memory      3-15ms
Physical Memory     100-150ns
2nd-Level Cache     40-60ns
1st-Level Cache     5-10ns
Registers           1ns

Between main memory and cache, memory is transferred in blocks known as *cache lines* which range in size.
Between virtual memory and main memory, data is transferred in blocks known as *pages*, typically measured in KB (at the time of writing)

See: https://en.wikipedia.org/wiki/Virtual_memory

7.4.3 Locality in Programs

Most programs exhibit high locality, meaning they only touch a small fraction of code and data.

A program has *temporaral locality* if the memory locations it accesses are likely to be accessed again in a short period of time
A program has *spatial locality* if the memory locations likely to be accessed are close to each other

The convential wisdom is that programs spend 90% of their time executing 10% of the code. Here's why:

- Only a small fraction of the code that could be invoked is actually executed, because many routines go unexecuted and many branches untaken
  in a typical run of the program.
- A typical program spends most of it's time in the innermost loop

** Locality allows us to keep the most commonly used data and instructions in the fastest cache tiers at all times **

For Ex. Say I've got a parser table stored in a contiguous memory block. How would I measure if this resides in a cache while the parser executes?

*Dynamic* Memory (DRAM) loses it's charge and needs to be periodically refreshed
*Static* Memory (SRAM) is made from a more complex circuit that retains it's charge

Typically faster circuits are made from SRAM, while main memory is made from DRAM

With instructions, the past is a good predictor for the future. If an instruction is recently executed,
then there's a high probability that it will again be executed.

This is an example of spatial locality at work. To improve it, we can place blocks that are executed frequently adjacent to eachother in memory.

To leverage cache efficiently, it's better if we can move data close together in memory, so that it can be read into cache from main memory,
and then to perform as many computations as possible on that cached memory before flushing it back to main memory.

Where in a cache a line is located can be restricted with *set associativity*
A cache is k-way set associative if a cache line can reside only in k locations
The simplest cache is a 1-way associative cache, AKA *direct-mapped*
- with direct-mapped cache, data with address n is placed at n mod s where s is the cache size
- k-way set associative cache is subdivided into k sets, and memory is mapped at n mod (s / k) in each set
- most instruction and data caches have associativity between 1 and 8.
- if none of the possible locations in each set is available, the least recently used cache line is evicted

See: https://en.wikipedia.org/wiki/Cache_placement_policies
See: https://en.wikipedia.org/wiki/CPU_cache

7.4.4 Reducing Fragmentation

At the beginning of execution, the heap is a contiguous block of free space.
As the program allocates and deallocates, the block will be broken down into smalelr units.
Chunks of free memory are called *holes*.
With each allocation, the memorhy manager must *place* the request chunk of memory into a large enough hole (thus shrinking the size of the hole).
Unless the hole is the same size as the request, we then need to *split* the hole.
With each deallocation, chunks of freed memory are added back to the pool.
We *coalesce* contiguous holes into larger holes, as the holes get smaller otherwise
If we're not careful, free memory gets *fragmented*.

*Fragmentation* is when you arrive at a state where you've got a large number of small, noncontiguous holes
which has the net effect of making it more challenging to find a hole large enough to servicer a request,
and is thus undesirable.

Best-Fit and Next-Fit

Empirically it's been shown that a good strategy is to allocate space from the smallest available hole that is large enough to service the request
- This is known as *best fit*

An alternative, *next fit*, takes the first available hole that is large enough. In the simple case it'll be faster to locate a hole for allocation,
but when fragmentation is taken into account best fit has been shown to have better overall performance.

We can track the size of our free space chunks with *bins* where each bin keeps a list of holes of a certain size.
Doug Lea's memory manager aligns chunks on 8 byte boundaries, and there is a bin for each multiple of 8 byte chunks from 16 to 512 bytes.

- So if there's a bin that corresponds to the requested size, we can take a chunk from that bin.
- For sizes taht do not have a private bin, we find a bin larger than that size, and we look for either the next fit or best fit chunk within that bin.
    - Note that for a chunk larger than the request size, we need to split, and the remainder will be added to a bin of a smaller size
- If the bin is empty, we move up a size and try to find either the next or best fit in that bin.
- If we can't find a bin with a chunk big enough to service the request, then we reach for the "wilderness" chunk (which can pull memory from teh OS).
- We can improve spatial locality by using next fit along with best fit when consecutive requests are of the same size.

Coalescing

If the user deallocates a chunk, we may or may not prefer to coalesce it together with other chunks depending on how our bins are organized, and how many remaining chunks are in our bins.

*Boundary Tags*
- at the low or high end of a chunk, we maintain metadata about the state of the chunk.

Automatic GC can help eliminate fragmentation altogether

7.4.5 Manual Deallocation Requests

- If a programmer fails to delete memory before it can no longer be referenced in the
  program it is known as a *memory leak*
- Referencing deleted meory is known as a *dangling pointer dereference error*

It is critical that long-running programs not have leaks, as they can build up over time.
Leaks generally do not impact the correctness of the program, provided the operating system doesn't run out of memory to allocate.
Accessing an illegal address is another error that leads to bugs and security concerns.

Historical Note: Rational's Purify helps find memory access errors and memory leaks (similar to Valgrind)

Tools:

Reference counting
- keeping track of how many reachable references the program has to a block of allocated memory
- this has a problem with circular data structures
- expensive, because it presupposes that you can increment and decrement on every pointer creation and deletion.

Region based allocation
- when objects are created to be used only within some step of a computation we can allocation all such objects to a region
- and when the phase of computation has ended, we can delete the region all at once

7.5 Garbage Collection

Unreferencable memory is known as *garbage*

Garbage collection dates back to 1958 with the initial implementation of LISP.

Reference Counting
Trace-Based

Garbage collection is the reclamation of the chunks of storage holding objects that can no longer be accessed by the program.

Design Goals:
- The objects need to have a deep that can be determined by the GC at runtime.
- From the type we can determine the size of the object and whether or not that object contains references to other objects.
- We also assume that references are always to the beginning of an object's address space.
- A user program is a *mutator* in the context of garbage collection.
- A mutator acquires objects from the memory manager.
- A mutator may create and destroy references to an object.
- Objects become garbage when the mutator cannot *reach* them anymore.
- The garbage collector then hands the unreachable objects back to the memory manager.

Not all languages are good candidates for automatic garbage collection
- A language in which the type of any data component can be determined is *type safe*
- Languages where types are determined at runtime is said to be *dynamically typed*
- A language where you can't determine the type at either compile time or runtime is *unsafe*

Unsafe languages like C and C++ aren't great candidates for garbage collection.
- In unsafe languages pointer's can be manipulated arbitrarily (pointer arithmetic), and so it's
    difficult to enforce the invariant that references must point to the beginning of an object's memory
- Theoretically, a language that allows for pointer arithmetic could have a reference to any memory location at any time.
- A theoretically unsound GC does exist for C and C++

GC is computationally expensive.

Performance Considerations:
- Overall Execution Time: if slow, it will dramaticalluy impact the overall running time of the program
- Space usage: it must avoid fragmentation and make the best use of available memory
- Pause Time: GCs are notorious for causing mutators to pause for an extremely long time while they reconcile what's still in use, and what should be reclaimed.
    - we need to minimize the maximum pause time (maybe by splitting up and partially reclaiming)
    - in real-time applications the pause time may be untenable
- Program Locality: the GC controls the placement of data, and so unless it's carefully designed to maximize locality it may make matters worse

Objects of different characteristics may favor different treatments by the GC


*** As an example of tradeoffs: ***

The longer we wait to garbage collect with a trace based collector, the greater fraction of objects will be collected.
Objects often die young, so we wait to build up a good chunk of reclaimable objects,
    and this decreases the overall time spent collecting on average.
On the other hand, infrequent collection will increase the overall memory usage of the program,
    will decrease it's data locality, and will increase the length of pauses.

A reference counter adds a constant overhead to many of the mutators operations (for instance, those that create and destroy references),
    which may greatly slow down program execution speed.
But it won't create long pauses and is generally memory efficient.

Functional languages may create many more intermediate values to avoid mutation, and thus generate more garbage.
And languages like Java may create heap allocated objects regardless of lifespan, which also create lots of garbage.

7.5.2 Reachability

Data that can be directly accessed by the program without having to dereference a
pointer is known as the *root set*.

In Java, this consists of static field members, stack variables, or register variables.

Any object that is stored in a field of a reachable object is itself reachable.

Compiler optimization makes this a bit more complex.
- A compiler may keep referenced variables in registers (which are part of the root set)
- Even though type safe language programmers do not manipulate addresses, the compiler may do so for optimization
- Thus, registers may violate the invariant about pointing to the beginning of an allocated object

An optimizing compiler can do the following to find the correct root set:
- Retrict garbage collection to only certain code points in the program
- The compiler can provide the GC with metadata about where referenced objects are located
- It can ensure that there is a reference to every base address for all in-use objects

The four basic operations a mutator performs that can change the set of reachable objects are:
1. Allocations
    - the memory manager assigns a new block of storage and the initial reference to the mutator
2. Passing parameters, and returning values
    - each of these potentially creates new references when a reference object becomes a parameter or return value
3. Assignment
    - assignment has the potential to create a new reference if the right hand side is a reference
    - if the left hand side was originally a reference, that reference is now gone
4. Returns
    - if a procedure returns, the locals are popped off the stack, and if these are references then they're gone too.

With a reference counting garbage collector, we can keep track of when an object becomes unreachable
    when it's reference counter hits 0.
With a trace-based garbage collector, we need to discover our reachable objects in a *marking* phase, and then conclude
    that every object not in our marked set is unreachable.

An object must have a field for the reference count in a reference counting GC

**RefCounts**
Object Allocation: Set RefCount to 1
Parameter Passing: Increment RefCount
Reference Assignments: For u = v, if u is a reference Decrement u's Refcount, finally Increment v's RefCount
Procedure Returns: Decrement the RefCount for all variables in the procedure's stack frame
Transitive Loss of Reachability: Whenever a RefCount reaches 0 we need to Decrement the RefCount for all of the objects it points to

Two Disadvantages:
- It cannot collect unreachable cyclic data structures, unless the programmer explicitly breaks the cycle,
    or the GC uses a hybrid mark/sweep approach
- It's expensive
- The overhead is high because additional operations are introduced for each of the above cases mentioned in **RefCounts**

Advantages include:
- Collection can be performed incrementally
- Modifying RefCounts while collecting can be deferred and spread across multiple pauses
- The ability to control the precise amount and number of pauses is attractive for UI and realtime applications

7.6 Trace Based Garbage Collection

Mark and Sweep
1. Mark all reachable objects
2. Sweep the heap to free unreachable objects

Trace-Based algorithm common states:
1. A chunk is in the *free* state if it is ready to be allocated
2. Chunks are unreachable unless proven reachable by tracing, and are in the *unreached* state.
3. Chunks that are reachable are either *unscanned* or scanned during the tracing process.
4. Unscanned objects become *scanned* when we process adding the pointed to objects it contains to the unscanned list.

Free -> Unreached
Trace Begins
Unreached -> Unscanned -> Scanned
Trace Ends
Scanned -> Unreached -> Freed

The final step of mark and sweep (the sweep) is expensive because there's no easy way to find the unmarked unreachable items without
examining the entire heap.

One improvement, by Baker, keeps a list of allocated objects.
Then we can take the set difference of allocated / reachable.

AKA "Baker's Mark and Sweep Collector"

Chunks returned to the free list remain as they were before deallocation, but to avoid fragmentation
we may seek to merge chunks together.

7.6.4 Mark and Compact Garbage Collectors

*Relocating* collectors move reachable objects around in the heap to eliminate fragmentation.

***
It is common that space occupied by reachable objects is much smaller than freed space,
and a key optimization of a relocating collector is that it may choose to relocate all of the
reachable objects to one end of the heap, instead of individually freeing and merging unreachable chunks.
***
* And as an added bonus, instead of maintaining a list of free chunks, we just need a single pointer to the start of the single huge free chunk.

As you discover reachable objects, compute the relocation address and store it in a table,
then relocate reachable objects.

Reference Counting /
Mark and Sweep /
Mark and Sweep w/ Relocation (Compact) /
Mark and Sweep w/ Copying

7.6.5 Copying Collectors

A copying collector reserves ahead of time a space to which the objects can move,
which absolves the collector of having to find free space while tracing.

Memory space is partitioned into two semispaces, A and B. When the GC pauses and copies, the roles of each reverse.
The mutator allocates objects in one space or the other, depending on role.

Cheney's Copying Collector

7.7 Short-Pause Garbage Collection

Trace-Based allocators use *stop-the-world* style garbage collection, which
has the unwanted side effect of sometimes introducing long pauses to user programs.

We reduce the length of pauses by doing the work incrementally or partially.

In *partial collection*, we collect only a subset of the garbage at a time, and reduce the space requirements of the garbage collector during any given pause.
in *incremental collection*, we interleave the work of collection and allowing the mutator to return, thus reducing the time requirements of the garbage collector during the pause.

Incremental collection breaks up reachability analysis into smaller units, and interleaves mutator execution. The mutator can modify what is/isn't reachable at any time, so this is complex.
The best known partial collection algorithm is *generational garbage collection*.

A *generational garbage collector* partitions allocated objects based on age, and attempts to collect on newer objects before older objects (because new objects tend to have shorter life times).

There's also the *train algorithm* for partial collection.

***
A garbage collector must NOT collect objects that are not garbage.
But a *conservative* garbage collector, may choose not to collect all of the objects that are garbage in a given round.
***

Garbage left behind with a *conservative* garbage collector is known as *floating garbage*.

Incremental *conservative* garbage collectors play it safe by overestimating the reachability of objects in a given round.

An incremental collector atomically adds the root set to the unscanned list, and then interleaves mutator execution with scanning for reachability.
Any actions taken during mutation that change the reachable set are recorded in a side table, and this side table is used to make necessary adjustments
as the garbage collector continues execution.
The expense comes in when the mutator loses the ability to reference an object. It would be expensive to re-establish reachability between pauses, so generally this floating garbage slips through the cracks.
New objects allocated between pauses can immediately be added to the unscanned list with very little expense.

***
The *conservative* part refers to the fact that it may not determine the reachability of references that have been dropped between incremental pauses, and then may leave behind some floating garbage.
***

7.7.2 Incremental Reachability Analysis

The problem is that actions of the mutator can violate the invariant of the tracing algorithm:
A _scanned_ object can only contain references to other _scanned_ or _unscanned_ objects, and never to _unreachable_ objects.

A more precise trace can be implemented by modifying the actions that the mutator takes that have the ability to impact reachability:

1. Write Barriers
   Intercept writes to references into a _scanned_ object, when the reference
   is to an unreached object. I.e. mark it for rescanning.
2. Read Barriers
   Intercept reads to references in Unreached or Unscanned objects, and mark the object
   as reachable and transition it to unscanned (ready to be scanned) state.
3. Transfer Barriers
   Intercept the loss of the original reference in the Unreached or Unscanned object

None of these finds the smallest set of reachable objects.

Implementing Write Barriers:
Remember all new references written into scanned objects.
It's up to the implementer to remember just how much detail they want about what regions of the address space contain "dirty" objects between mutations.

80-90% of all newly allocated objects die young, so it's best to structure your GC algorithm around that.
Objects that survive a collection once are more likely to survive successive collections.
Generational garbage collection works on the area of the heap that contains the youngest objects.
Once a small object has become sufficiently mature we might want to move it to a separate area of the heap and manage it with another algorithm, like the "train" algorithm.

A set of objects to be collected on a round of partial collection is referred to as a *target set*.

*Generational* garbage collection is an effective way to exploit the property that most objects die young.

Eventually collection of the oldest generation occurs, and this requires the full tracing step from a standard mark and sweep collector,
and so generational collectors may introduce long pauses.

The train algorithm handles only mature objects instead of all objects during a collection as a means of reducing the pause.

7.8 Advanced Topics in GC

1. GC in parallel environments
2. Partial relocations of objects
3. Garbage collection for languages that are not type-safe
4. Manual vs GC interaction

In a parallel environment each individual thread becomes a mutator.
In a parallel environment the root set becomes larger, because every thread has it's own stack and register set, and potentially global variables.

The set of objects reached by the root can be very large, and so it may be infeasible to compute reachability while all mutations case.

So we use garbage-collecting threads concurrently with mutator threads to trace most of the reachable objects.
Then stop all of the mutators, and quickly find the remaining.

Outline:
1. Scan the root set for each mutator thread, and put all objects directly reachable
   into the unscanned state.
2. Scan objects in the unscanned state. Use a work-queue of fixed-size work packets.
   Threads will then dequeue the work packets, and trace the unscanned objects within.
   If the system runs out of space we simply mark the cards holding the objects and force scanning.
   Because we're out of space, we can't allocate work packets, but the bit array for determining dirty cards has already been allocated.
3. Scan the objects in dirty cards. When interleaving this with mutation we need some criterion to determine when to stop the trace.
4. Stop all mutators, and use all threads to do a final rescan and.

Unsafe Languages
Because languages like C and C++ can always compute addresses with arithmetic operations .
A conservative collector that works with such a language assumes we cannot fabricate an address,
or derive the address of an allocated chunk of memory without an address pointing somewhere in the chunk.
We then treat as a valid address any bit pattern found anywhere in reachable memory, as long as that bit pattern may be interpreted as a memory location.
Object relocation is incompatible with this form of conservative garbage collection,
    because it can't know for sure if a particular bit pattern is an address that may need to be updated to point to a relocated object.

Conservative GC with type unsafe languages:

1. Memory manager keeps a *data map* of all of the allocated chunks of memory,
   which allows us to find the starting and ending boundary of a chunk of memory that spans a certain address.
2. Tracing starts by scanning the program's root set to find any bit pattern that looks like a memory location,
   without worrying about it's type, and if any of these bit patterns point to allocated chunks we conclude that that chunk is _reachable_.
3. Then we scan all of the _reachable_ and _unscanned_ objects to find more reachable chunks that we then place on the work queue.
4. We then sweep through the heap storage using the data map to locate and free all unreachable chunks of memory.

TODO: read more about weak references




