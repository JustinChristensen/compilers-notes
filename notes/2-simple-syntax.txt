2. A Simple Syntax Directed Translator

fragment:
{
    int i;
    int j;
    float[100] a;
    float v;
    float x;

    while (true) {
        do i = i+1; while (a[i] < v);
        do j = j-1; while (a[j] > v);
        if (i >= j) break;
        x = a[i]; a[i] = a[j]; a[j] = x;
    }
}

intermediate representation:
1. i = i + 1
2. t1 = a[i]
3. if t1 < v goto 1
4. j = j - 1
5. t2 = a[j]
6. if t2 > v goto 4
7. ifFalse i >= j goto 9
8. goto 14
9. x = a[i]
10. t3 = a[j]
11. a[i] = t3
12. a[j] = x
13. goto 1
14.

Context Free Grammar
Backus-Naur Form

Infix Form:
9 - 5 + 2

Postfix Form:
9 5 - 2 +

Lexical Analyzer - Splits input into basic units for syntax analysis called tokens
Intermediate Code Generation -
    Syntax Trees, a representation of the source program
    Three Address Code, an intermediate structure

Three Address Code:
Named because it's instructions are of the form x = y `op` z
Where x, y, and z are the addresses of the operands and the result

Production rules:
An if expression of the form:
if (expression) statement else statement

Can be described using the following production:
stmt -> "if" '(' expr ')' stmt "else" stmt
"if", '(', ')', and "else" are terminal symbols
stmt and expr are non-terminal symbols

A CFG has four components:
1. Terminals
2. Non-Terminals
3. Productions
4. A Start Symbol

For example:
list: list '+' digit
    | list '-' digit
    | digit

terminals:
+ - 0 1 2 3 4 5 6 7 8 9

epsilon:
ε - string of zero terminals, the empty string

Parse Trees:
A: X Y Z
        A
  +---  |  ---+
  |     |     |
  X     Y     Z

A: X Y
 | Z
        A               A
  +---     ---+         |
  |           |         |
  X           Y         Z

ambiguity
a grammar can have more than one parse tree for a string of terminals

** to show that a grammar is ambiguous, all we need to do is show that a terminal string
   is the **yield** of more than one parse tree **

for example:
string: string + string
      | string - string
      | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9

        string                   string                           string
    +--    |   ---+          +--    |   ---+                         |
    |      |      |          |      |      |                         |
 string    +   string     string    -   string     0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9

associativity of 9 - 5 + 2

Parse Trees
(9 - 5) + 2
                       string
           +----------    |   ---+
           |              |      |
        string            |   string
    +--    |   ---+       |      |
    |      |      |       |      |
 string    |   string     |      |
    |      |      |       |      |
    |      |      |       |      |
    9      -      5       +      2          <--- the "yield" of terminals

9 - (5 + 2)
        string
    +--    |   -----------+
    |      |              |
    |      |           string
    |      |      +---    |   ---+
    |      |      |       |      |
 string    |   string     |   string
    |      |      |       |      |
    |      |      |       |      |
    9      -      5       +      2          <--- the "yield" of terminals

Ex. 2.2.1
S : S S '+'
  | S S '*'
  | 'a'
a) aa+a*
    1. a = S: 'a'
    2. b = S: 'a'
    3. c = S: a b '+'
    4. d = S: 'a'
    5. e = c d '*'

Syntax Directed Translation
- Attributes
- Translation Schemes

Syntax Directed Definition Associates:
1. For each grammar symbol, a set of attributes
2. For each production, a set of semantic rules for puting the values of the attributes

X = the label for the grammar symbol at a particular node
X.a = attribute a for grammar symbol X

expr = 9 - 5 + 2
expr.t = 95-2+

                    expr.t = 95-2+
           +----------    |   ---+
           |              |      |
      expr.t = 95-        |   term.t = 2
    +--    |   ---+       |      |
expr.t = 9 |      |       |      |
    |      | term.t = 5   |      |
term.t = 9 |      |       |      |
    |      |      |       |      |
    9      -      5       +      2

Infix to Postfix:

Production              Semantic Rules
expr: expr₁ + term      expr.t = expr₁.t <> term.t <> +
expr: expr₁ - term      expr.t = expr₁.t <> term.t <> -
expr: term              expr.t = term.t
term: 0 | 1 | ...       term.t = 0 | 1 | ...

Tree Traversals:
Pre-Order
Post-Order
In-Order

Depth vs Breadth

Any bottom-up order can be used for a syntax directed definition, because node N _depends_ on it's children,
and so simply evaluating the children in any order is good enough.

Pre vs Post
void visit(node) {
    // pre-order
    for (int i = 0; i < length(node->children); i++) {
        visit(node->children[i]);
    }
    // post-order
}


Semantic Actions:
expr: expr₁ + term { print('+') }
expr: expr₁ - term { print('+') }
expr: term
term: 0 | 1 | ...  { print('N') }


                      +------------------------------------  expr  ----------------------------------+
                      |                                        |  |                                  |
                      |                                        |  |                                  |
                    expr  -----------------------------+       |  +--------------+                   |
       +----------    |   ---------+                   |       |                 |                   |
       |              |            |                   |       |                 |                   |
       |              |            |                   |       |                 |                   |
     expr             |          term                  |       |               term                  |
       |              |            |                   |       |                 |                   |
       |              |            |                   |       |                 |                   |
     term             |            |                   |       |                 |                   |
       |              |       +----+                   |       |            +----+                   |
  +----+              |       |    |                   |       |            |    |                   |
  |    |              |       |    |                   |       |            |    |                   |
  |    |              |       |    |                   |       |            |    |                   |
  9    |              -       5    |                   |       +            2    |                   |
       |                           |                   |                         |                   |
       |                           |                   |                         |                   |
{ print ('9') }             { print ('5') }     { print ('-') }           { print ('2') }     { print ('+') }


**Semantically, this prints the translation as the tree is traversed, whereas
the above semantics (with <>) builds up a string to be a printed.**

We could also have built up a full AST if we needed to. Lots of possibilities with semantic actions.

AST Example:
data Expr
    = Add Expr Term
    | Sub Expr Term
    | Term Int

Parsing
The process of determining how a string of terminals can be generated by a grammar

Recursive Descent Parsing
- One procedure for each non-terminal symbol, generally

Worst case scenario for a Parser for a Context Free Grammar is O(n^3) for a string of n terminals
For real languages, Context Free Grammars are designed to be much faster than this. The key
being that the Context Free Grammar determines the speed at which parsing is possible.

** TODO: Find out what kind of grammar takes O(n^3) steps to parse **

Parsing usually falls into two categories:
top-down - construction starts at the root and proceeds towards the leaves
bottom-up - construction starts at the leaves and proceeds up towards the root

top-down example:
start: stmt

stmt: expr ';'
    | "if" '(' expr ')' stmt
    | "for" '(' optexpr ';' optexpr ';' optexpr ')' stmt
    | "other"

optexpr: ε
       | expr

lookahead symbol:
initially the first terminals of a string
then successive terminals after that

The process of trying production rules may involve backtracking:

For terminal string:
for ( ; expr ; expr ) other
 ↑  // lookahead

// doesn't begin with "expr", backtrack 1
-> "expr" ';'
   "if" '(' "expr" ')' stmt
   "for" '(' optexpr ';' optexpr ';' optexpr ')' stmt
   "other"

// doesn't begin with "if", backtrack 1
   "expr" ';'
-> "if" '(' "expr" ')' stmt
   "for" '(' optexpr ';' optexpr ';' optexpr ')' stmt
   "other"

// sweet, we found a production that begins with "for"
// try match the rest
   "expr" ';'
   "if" '(' "expr" ')' stmt
-> "for" '(' optexpr ';' optexpr ';' optexpr ')' stmt
   "other"

Predictive Parsing
The simplest form of recursive descent parsing, where the lookahead symbol
is guaranteed to uniquely determine the flow of control. The sequence of procedure
calls forms an implicit parse tree that can be used to form an explicit one, if the semantic
actions will it so.

match(for)
match('(')
match(optexpr) -> match(ε)
match(';')
match(optexpr) -> match(expr)
match(';')
match(optexpr) -> match(expr)
match(')')
match(stmt) -> match(other)

every time we match a terminal we can advance the lookahead symbol
e.g. if (match(lookahead, ';'))
        lookahead = next();

a _translation scheme_ is formed by extending a _grammar_ (adding semantic actions to it)
a _syntax directed translator_ is formed by extending a _predictive parser_

left recursive productions can cause a recursive-descent parser to loop forever:
expr: expr + term
void expr() {
    expr()
    match(+)
    term()
}

Conflict:
We need a grammar that facilitates translation, and a different grammar that facilitates parsing

expr: expr + term { print('+') }    <-- left recursive, would loop forever in a top down parser
    | expr - term { print('-') }
    | term
term: 0 | 1 | ...  { print('N') }

ASTs:
Abstract syntax trees, or syntax trees, resemble parse trees to an extent, but with the important
difference that interior nodes represent constructs instead of the precise non-terminal. In an AST,
the terminal symbols are implied by the structure:

-- single non-terminals like term are replaced with their terminals
-- AST
data Expr
    = Add Expr Int     -- note the missing terminal, +, and the fact that Term has been replaced with Int
    | Sub Expr Int     -- note the missing terminal, -  and the fact that Term has been replaced with Int
    | Term Int

with left recursion elimination the above grammar becomes:
expr: term rest
rest: + term { print('+') } rest
    | - term { print('-') } rest
    | ε
term: 0 | 1 | ...  { print('N') }

** NOTE that none head symbols of any of the production rules match the
non-terminal on the left hand side, so the left recursion has been eliminated **

************ AVOIDING INFINITE LOOPS IN TOP DOWN PARSERS ************
Left Recursion Elimination (adds a new non-terminal, R, to the grammar):
A: Aa
 | Ab
 | c
   ↑ <- left recursive on non-terminal A, infinite loop
   {A,c}

becomes:
A: cR
R: aR
 | bR
 | ε
   ↑ <- no head symbols matching the lhs non-terminal
   {c,a,b,ε}
************ AVOIDING INFINITE LOOPS IN TOP DOWN PARSERS ************

Simplifications

1. Eliminating Tail Recursion
    void rest() {
        if (lookahead == '+') {
            match('+'); term(); print('+'); rest();
        }
        elseif (lookahead == '-') {
            match('-'); term(); print('-'); rest();
        }
        else {}
    }

    becomes:

    void rest() {
        while (true) {
            if (lookahead == '+') {
                match('+'); term(); print('+');
            }
            elseif (lookahead == '-') {
                match('-'); term(); print('-');
            }
            break;
        }
    }

    because the tail-recursive calls are identical to a goto to the beginning
    of the procedure, rest.

2. Removing the rest non-terminal procedure
    void expr() {
        term();
        rest();
    }

    becomes:

    void expr() {
        term();
        // body of the latter rest definition
        while (true) ...
    }

Lexical Analysis
Grouping characters from the input stream into tokens, the basic units for parsing
Lexeme - sequence of characters that comprises a token

Adding ids, multiplication, and division:
Order dictates how tightly operators bind to their operands
expr: expr + term   { print('+') }    <-- expr bind least tightly
    | expr - term   { print('-') }
    | term
term: term * factor { print('*') }
    | term / factor { print('/') }
    | factor
factor: ( expr )
      | num         { print(num.value) }
      | id          { print(id.lexeme) }


* White space generally eliminated by the lexical analyzer, incorporating it into
the languages grammar would be a huge pain in the ass
TODO: look at the formal grammars for whitespace sensitive languages like Python
https://docs.python.org/3/reference/grammar.html
funcdef: 'def' NAME parameters ['->' test] ':' suite
suite: simple_stmt | NEWLINE INDENT stmt+ DEDENT

* Comments are generally also eliminated by the lexical analyser, for the same reason
as above

* Note that the above is only sort of true for top down parsers written in languages
that support higher order functions, because the procedures are relatively easy to bake
whitespace and comment elimination into:
letT = elimWs(keyword("let");
runParser letT "let // let statement"

constants like "foo", 'c', and 134234234 are generally treated as single units in the grammar

keyword and identifiers can look similar, in languages that allow keywords as identifiers context
becomes extremely important (and so it must be handled by the parser):
isKW k -> k in { "let", "in", "var" }
id = [a-zA-Z_]\w+

expr: "let" decls "in" expr
    | "var" id

parse expr "var let" <--- let would be treated as an identifier here

Symbol Tables
data structures used by compilers to hold information about source-program constructs
- essentially a bridge from the later phases of the compiler to the source program syntax as
  it was written by the author
- usually a hash table, could be other things, should probably support efficient lookup
- could be a stack of symbol tables if scope is needed
- parser has the context to know whether to create or update a symbol table entry
- without a stack, and with a global hash table, entries have to be created or destroyed upon block
  entrance or exit, respectively.

most closely nested rule:
find x starting from the top of the stack and working outwards

block stack example (using a pair of globals, for max stack size 1):
    struct env *top, *saved;

    prog: block             { top = null }
    block: '{'              { saved = top; top = new_env(); }
            stmts '}'       { top = saved; }
    stmts: stmts stmt
    stmt: block | decl
    decl: type id           {
                                struct symbol *sym = new_sym();
                                sym->id = $2
                                sym->type = $1
                                top->put(sym}
                            }

Two kinds of Intermediate Representations for this chapter:
Syntax Trees - allows for better optimization in later passes
Three Address Code

Translation Scheme
program: block                      { program.n = block.n }
block: '{' stmts '}'                { block.n = stmts.n }
stmts: stmts' stmt                  { stmts.n = seq(stmts'.n, stmt.n) }
     | ε                            { stmts.n = null }
stmt: expr ';'                      { stmt.n = eval(expr.n) }
    | if '(' expr ')' stmt'         { stmt.n = if(expr.n, stmt'.n) }
    | while '(' expr ')' stmt'      { stmt.n = while(expr.n, stmt'.n) }
    | do stmt' while '(' expr ')'   { stmt.n = do(expr.n, stmt'.n) }
    | block                         { stmt.n = block.n }
expr: rel = expr'                   { expr.n = assign(rel.n, expr'.n) }     <---- but is it grammatically correct to have a relation on the lhs of an assignment?
    | rel                           { expr.n = rel.n }
rel: rel' '<' add                   { rel.n = lt(rel'.n, add.n) }
   | rel' "<=" add                  { rel.n = lteq(rel'.n, add.n) }
   | add                            { rel.n = add.n }
add: add' '+' term                  { add.n = add(add'.n, term.n) }
   | term                           { add.n = term.n }
term: term' '*' factor              { term.n = mult(term'.n, factor.n) }
    | factor                        { term.n = factor.n }
factor: '(' expr ')'                { factor.n = expr.n }
      | num                         { factor.n = num(num.value) }

See: sdt.txt in this directory

The number of non-terminals for expressions will be equal to the number of precedence
levels plus one

Static Checks:
- ensuring that an identifier is declared at most once in a scope (grammar can't ensure that)
- type checking, coercion

Three Address Code
generally, x = y `op` z

Arrays:
x[y] = z
x = y[z]

Conditionals:
ifFalse x goto L    <-- label "L"
ifTrue x goto L
goto L

Labels:
L: x = y `op` z
goto L

Assignment:
x = y

Syntax -> Three Address:
a[i] = 2 * a[j-k]

Generates:
t3 = j - k
t2 = a[t3]
t1 = 2 * t2
a[i] = t1


Given the left recursive grammar:
add: add + num
   | num

And:
An abstract syntax tree is just a parse tree with extra syntactic noise removed.

        add
       /   \
     add    \        <-- lower binds more tightly, i.e. (3 + 4) + 5
     /  \    \
   add   \    \
    |     \    \
    3  +  4  +  5

Problem:
Eliminating left recursion leads to a parse tree that doesn't reflect the left associativity semantics of
a left-recursive binary operation:
add: num add'
add': + num add'
    | e

   add
    |   \
    |    add'
    |     |   \
    |     |    add'         <-- lower binds more tightly, i.e. 3 + (4 + 5)
    |     |     |  \
    |     |     |   add'
    |     |     |    |
    |     |     |    e
    3  +  4  +  5

https://cs.stackexchange.com/questions/43066/removing-left-recursion-in-grammar-while-maintaining-left-association-of-operato

       lteq_rel
       /    \
     lt_rel  \
    /   \     \
add_rel  \     \
  |       \     \
  |       |      \
 add  <  add  <=  add
