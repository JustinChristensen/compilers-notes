Instruction-Level Parallelism

pipelining
- a CPU can issue several operations in a single clock-cycle
- or it can issue just one, and pipeline the operations
- practically every processor uses an instruction pipeline
- a new instruction can be fetched every clock while preceding instructions are still going through the pipeline

pipelining cont
- if the result from an instruction is available by the time the succeeding instruction needs the data, the processor can issue an instruction each tick
- branch instructions are problematic because, until they are fetched, decoded, and executed, the processor does not know which instruction will execute next
- for a 5-stage instruction pipeline instructions i, i + 1, i + 2, i + 3, and i + 4 are executed in parallel (sort of)
- many processors speculatively fetch and decode the immediately succeeding instructions in case a branch is not taken
- when a branch is taken, the instruction pipeline is emptied and the branch target is fetched
- example 5 stages: IF (fetch), ID (decode), EX (execute), MEM (memory access), WB (write back result)

execution
- some instructions take several ticks to execute, like memory-load
- an instruction is _pipelined_ if succeeding instructions not dependent on the result are allowed to proceed
- if the deepest executin pipeline has n stages, then n operations can be "in flight" at the same time
- not all instructions are fully pipelined
- for example, floating point adds and multiplies usually are fully pipelined, while divides are not
- general purpose processors dynamically detect dependencies between consecutive instructions, and automatically stall the execution of instructions if their operands are not available
- some processors, like those in embedded devices, leave dependence checking to software to keep power consumption low
- machines that rely on software to manage instruction parallelism are known as VLIW (very long instruction word) machines, and those that manage it in hardware are known as _superscalar_ machines.
- compilers manage parallelism in VLIW machines by inserting special instructions
- superscalar machines have a regular instruction set with sequential execution semantics, but automatically detect dependencies and parallelize

Code Scheduling Constraints

code generated by a code generator can be _scheduled_ which is a form of optimization and is subject to three constraints:
1. control-dependence constraints, i.e. all of the operations executed in the original program must be executed in the original one
2. data-dependence constraints, i.e. the operations in the optimized program must produce the same results as the corresponding ones in the original program
3. resource constriants, i.e. the schedule must not oversubscribe resources on the machine

operations that write to the variables that another operation depends on are said to share a _data dependence_ and cannot be commuted
three flavors:
1. _true dependence_ read after write, if a write is followed by a read of the same location, the read depends on the value written
    o1: z = x + y
    o2: r = !z          <-  o2 depends on the result of o1
2. _antidependence_ write after read, if the write happens before the read, then the read operation will pick up the wrong value
    o1: z = x + y
    o2: x = #1          <- if this occurs before o1, then we'll overwrite the value of x that z should be dependent on
3. output dependence, write after write, two writes to the same location share an output dependence
    o1: z = x + y
    o2: z = #10         <- should these instructions result in z being x + y or 10?

#2 and #3 are referred to as _storage-related dependencies_.
- they can be eliminated by choosing different locations for the writes

Languages that support pointers make it challenging to prove that there are no data dependencies between instructions

data-dependence analysis:
- determining whether or not operations refer to disjoint memory locations

pointer-alias analysis:
- two points are _aliased_ if they can refer to the same object
- analyzing this is difficult because there are potentially many aliased pointers in a program

interprocedural analysis:
- determine, for example, if the same variable is passed as two or more different arguments

The goal of minimizing the number of registers used conflicts directly with the goal of maximizing instruction-level parallelism

hardware register renaming is a stopgap computer architests put in place to undo the effects of register optimization in compilers
- it eliminates false data dependencies that arise from teh reuse of registers by successive instructions that do not have any real data dependencies between them
- example:
    LD t1 a
    ST b t1
    LD t2 c
    ST d t2
- if the temporary registers t1 and t2 are allocated to distinct registers, than this code can be parallelized, otherwise it must execute serially
- traditional register-allocation techniques seek to minimize the number of registers used to perform any one computation
- see example 10.3 for how this in effect serializes a computation, and prevents it from being executed in parallel

Scheduling within a basic block is easy, because all of the instructions within the basic block are guaranteed to execute once we've entered the block
- they can also be reordered arbitrarily as long as the data dependencies are satisfied
- but basic blocks are typically very small, and their instructions are highly dependent
- So it's critical to think bigger
- If we know that an instruction is _likely_ to be executed, and we've got an idle resource available, we can execute the instruction _speculatively_
- The program then runs faster if the speculation turns out to be correct

An instruction j is said to be _control-dependent_ on instruction i if the outcome of i determines whether or not j is executed
E.g.
    while (c) s;    <-- s is control-dependent on c

Because memory loads are costly, it would seem that they'd be fertile ground for speculative execution, but it's possible for speculatively executing a load may raise an illegal address exception, or a page fault, or cache miss
    and so this is generally not done.

Special features to speculatively access memory:
prefetching
    prefetch instruction brings memory to the cache before it is used, if the access would be invalid or cause a page fault, the processor can simply ignore it
poison bits

Predicated Execution
- Branches are expensive, and mispredicted branches are even moreso.
- predicated instructions, can reduce the number of branches in a program
- like a normal instruction, but with an extra predicate operand
- CMOVZ R2 R3 R1 "move R3 to R2 only if R1 is zero"

For example:
    if (a == 0)
        b = c + d;
To:
    LET a = R1, b = R2, c = R4, d = R5 in
    ADD R3 R4 R5
    CMOVZ R2 R3 R1

- without the JMP statements this can become part of a larger basic block, and the processor doesn't have a chance to mispredict

Static vs Dynamically Scheduled machines:
- statically schedule machines explicitly define what can execute in parallel in the instruction set
- dynamically scheduled machines analyze data dependencies and schedule on-the-fly, and this is not reflected in the instruction set

Basic Machine Model:
M = (R, T)
1. A set of operation types, T, such as loads, stores, arithmetic, etc
2. A vector R = [r1, r2, ..., rn] representing hardware resources, where ri is the number of units available of the ith kind of resource
   Resource types include: memory access units, ALUs, FPUs, etc
- each operation has a set of input operands, a set of output operands, and a resource requirement
- each input operand has an associated input latency, which is a requirement on when the input must be available
- and each output opernad has an output latency, which indicates when the result will be available

resource usage for each machine operation type t is modeled by a two dimensional _resource-reservation table_, _RT_.
- the width of the table is the number of kinds of resources in the machine, and it's length is the duration over which resources are used by the operation
- Entry RT(t)[i, j] is the number of units of the jth resource used by an operation of type t, i ticks after it is issued

Basic Block Scheduling
- _list scheduling_ for scheduling operations within a basic block
- a data dependence graph G = (N, E) where the nodes are operations n in N and
- the edges are labeled with a delay d, indicating the detination node must be issues no earlier than de clocks after the source node is issued

Fully pipelined operations are operations that can be issued every tick, even though their results are not available until some number of clocks later

OP dst src1 src2
LD dst addr
ST addr src

So within a given sequence of instructions, there are potentially multiple valid orders based on data dependencies and resource availability, and the challenge
    is to pick one of those orders. AKA "scheduling"

See figure 10.9 for an example output schedule, consisting of a table where columns are indexed by resources (alu, mem), and rows are the instructions being executed
- if instructions dependent on distinct resources, then they can be executed "at the same time"
- the "critical" path is the longest path through a data-dependence graph

Global scheduling requires that we consider not only data dependencies, but also control dependencies
- all instructions in the original program are executed in the optimized program
- optimized program may execute extra instructions speculatively as long as they don't have unwanted side effects
- generally speaking, the end goal for a set of blocks is to arrive at a state where the loads happen before operations and branches, and then to execute the appropriate
    store operation based on the result of the branching instruction

TODO: think about what it means for an instruction to have "side effects"

SEE ALSO: Intel Optimization Guide:
https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf
https://en.wikichip.org/w/images/7/7e/skylake_block_diagram.svg

There's references in the above to the cost of individual instructions and the number of cycles it takes to access L1, L2, and L3 caches.
OSX CPU brand: sysctl -a machdep.cpu.brand_string

Compilers have techniques to estimate execution frequencies
The conventional wisdom says that 90% of a program's execution time is spent executing 10% of the code
- Reasonable to assume that instructions in innermost loops are executed more frequently
- Also reasonable to assume that backwards paths from branches are executed more frequently
- Best technique for frequency estimates is dynamic profiling, wherein you instrument the code to record paths taken, and then test it out

Nothing Follows

Loop unrolling:
    for (i = 0; i < N; i++)
        S(i);
To
    for (i = 0; i + 4 < N; i += 4)
        S(i), S(i + 1), S(i + 2), S(i + 3);
    for (; i < N; i++)
        S(i);

This creates more instructions within the loop body, and helps with scheduling

TODO: look into dynamic schedulers again

Branch misprediction is an important cause of loss of performance
Because of the long misprediction pentalty, instructions on rarely executed paths can still have a significant effect on the total execution time

Static - Compiler chooses order of execution
Dynamic - Hardware chooses order of execution
Again, VLIW vs Superscalar
Out of order execution
See: https://www.cs.umd.edu/~meesh/cmsc411/website/projects/dynamic/intro.html#:~:text=Dynamic%20scheduling%2C%20as%20its%20name,determines%20the%20order%20of%20execution.&text=(All%20of%20this%20is%20assuming,the%20same%20instruction%20set%20architecture.




