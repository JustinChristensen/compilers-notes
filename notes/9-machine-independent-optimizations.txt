Global optimizations are usually based on *data flow analysis*
- they usually have the same form: for each instruction, some property that must host each time the instruction is executed

A few different types of data-flow analyses:

* constant propagation analysis
    - at each point in the program and for each variable in use, does that variable hold a unique constant value
    - we can use this information to replace variable references (with might involve loading the value from memory into a register) with constant values
* liveness analysis
    - at each point in the program, is the value held by a particular variable sure to be overwritten before it's read
    - we then have no need to preserve that value in a register or in memory
* hierarchical analysis
    - used to eliminate "induction" variables (variables that count the number of iterations around a loop)

It's all about gathering information globally about the program.

***A compiler MUST preserve the semantics of the original program.***

In general, a compiler cannot understand enough context about a programmer's algorithm and it's intent
to replace it with a substantially different or better algorithm. We're confined to low-level transformations
involving algebraic identities or semantic properties.

Redundancy elimination can and should be opaque to the programmer

A compiler can improve a program without changing what it computes by:
* common-subexpression elimination
    - if none of the values E depends on change between E's computation and the next time E is encountered, we can just re-use E.
* copy-propagation
    - u = v is a _copy statement_
    - by replacing subsequent uses of u in the block with v, we set the stage for removing the copy instruction altogether
* dead-code elimination
    - a variable is _live_ if it's value can be used subsequently, otherwise it's _dead_
    - _dead code_ is statements that computed a value that is never used
    - after applying copy propagation and constant folding we might arrive in a situation where a block
        is wrapped in an if (false) { ... which means the entire block is dead code and thus eliminatable.
* constant folding
    - deducing that the value of an expression is constant, and replacing the expression with the constant

These are _semantics preserving_ transformations

Loops are important for optimization. Moving code out of the inner loop, even if it bloats the outer code, will reduce the running time of the algorithm

Code motion:
    while (i <= limit - 1)      <-- limit - 1 executed many times
vs
    l = limit - 1               <-- limit - 1 executed once
    while (i <= l)

Induction Variables
Strength Reduction

Data Flow Analysis
- A body of techniques that derive information about the flow of data along program execution paths
- the execution of a program can be viewed as a sequence of transformations on the program _state_
- Each instruction results in a transformation from State -> State
- So we build up all of the possible sequences of states, and then extract the data we need from the states to perform the transformations

An _execution path_ from point (state) p1 to point pn to be a sequence of points such that for each
i = 1, 2, ..., n - 1 either:
1. p(i) is the point immediately preceding a statement, s, and p(i + 1) is the point immediately following s
2. p(i) is the end of a block, and p(i + 1) is the beginning of a successor block

In general, there's an infinite number of execution paths through a program and no defined upper bound on the length of a particular execution path.
Also in general, it is not possible to keep track of all of the program states for all possible paths.
- so we keep track of only what must be kept track of to perform the analysis that we're trying to do.

Definitions that _may_ reach a program point along some path are known as _reaching_ definitions.

Denotation for data-flow values before and after statements:
IN[s], OUT[s]

*** The data-flow _problem_ is to find a solution to a set of constraints on the IN[s] and OUT[s] for all statements s.***
Two sets of constraints:
1. Semantics of the statements ("transfer functions")
2. Flow of control

Data flow values before and after a statement are constrained by the semantics of the statement
The relation between state s and state t, f(s) = t is known as the _transfer function_
- We can also define the inverse transfer function g(t) = s

Within a basic block, control-flow constraints:
IN[s(i + 1)] = OUT[s(i)]
Between basic blocks:
If we're interested in collecting the definitions that may reach a program point,
    then the set of definitions reaching the leader statement of a basic block is the union of the definitions
    after the last statements of each of the predecessor blocks.

What goes on inside of a basic block is quite simple, so we can save time and space by defining the data-flow
    schema in terms of data values entering and leaving basic blocks

The transfer function for an entire block is the composition of the transfer functions of the statements the block contains:
f(B) = f(sn) . f(s(n - 1)) . ... . f(s2) . f(s1)

Thus, f(IN[B]) = OUT[B]

Reaching Definitions
A definition d _reaches_ p if there is a path from the point immediately following d to p,
    such that d is not _killed_ along the path.
A definition d is _killed_ if it is redefined along the path.
A _definition_ of x is a statement that assigns, or may assign, a value to x

Program analysis must be conservative: If we don't know whether or not a statement is assigning a value to the variable x, we must ASSUME that it does.

One trick for detecting variables used before they're defined:
- introduce dummy definitions for all variables that have been used.
- if one of those dummy definitions reaches a point where the variable is used without being
    killed by the real definition, then we've probably got a use-before-definition situation on our hands

To decide whether each path in a flow graph can be taken is undecidable
So we must assume that every path can be taken.

It is always unacceptable for an optimization to change what the program computes, and so optimizations must be conservative.

Constraints for the Reaching Definitions Problem:

d: u = v + w "generates" a definition of u and "kills" other definitions of u in the program

So the transfer function for this definition is:

f(x) = gen U (x - kill) where gen is { d } and kill is all of the other definitions of u in the program

This can be defined for statements by using the composition rule above.

Like a statement, a basic block also generates a set of definitions and kills a set of definitions.

Constraints for Control-Flow

The "meet" operator for reaching definitions is the confluence of different paths merging together in the flow graph.

***
The reaching definitions problem equations are then:
OUT[ENTRY] = {}
OUT[B] = gen(B) U (IN[B] - KILL(B))
IN[B] = U(P is a predecessor of B) OUT[P]
***

The Algorithm for Reaching Definitions
In:
    Flow graph for which the kill set and the gen set have been computed for each block
Out:
    IN[B] and OUT[B] the set of definitions "reaching" the entry and exit of each block

Least fixed-point. Start with OUT[B] = {} for all blocks, and then converge on the desired values

Until the set of OUT[B] for each block does not change:

OUT[ENTRY] = {}
for each basic block B other than ENTRY: OUT[B] = {}
while changes to the set of OUT[B] sets occur
    for each basic block B other than ENTRY
        IN[B] = UNION(each predecessor P of B: OUT[P])
        OUT[B] = genB U (IN[B] - killB)

If the basic blocks are properly ordered for the inner for loop, empirical evidence shows that the while loop will only iterate at most 5 times.

The book represents representing definition sets as bit vectors (and I agree).

Live-Variable Analysis

We wish to know whether the value of variable x at point p could be used along any of the paths leading from point p
If it can be used, we say the variable is _live_
Otherwise, it's _dead_

In:
    Flow graph with def and use computed for each block
Out:
    IN[B] and OUT[B] the set of variables live on entry and exit of each block B in the flow graph

IN[EXIT] = {}
for each basic block B other than EXIT: IN[B] = {}
while changes to the set of IN[B] sets occur
    for each basic block B other than EXIT:
        OUT[B] = UNION(each succcessor S of B: IN[S])
        IN[B] = useB U (OUT[B] - defB)

Available Expressions

Used for finding common subexpressions across blocks. Similar to the above.

Each data-flow problem is defined by:
1. The domain of the data-flow values
2. The direction of the data-flow
3. The family of transfer functions
4. The boundary condition
5. The meet operator

Figure 9.21 summarizes each for the reaching definitions, live variables, and availabe expression data-flow analysis problems

Data Flow Analysis Framework

A 4-tuple: (D, V, ∧, F)
1. the direction of the data flow, D
2. A semilattice consisting of the domain of values V and the meet operator ∧
3. Family F of transfer functions from V to V
    - must include functions suitable for boundary conditions, i.e. constant transfer functions

A *Semilattice* is a set V with a binary operator ∧ such that for all x, y, and z in V:
1. x ∧ x = x                        idempotency
2. x ∧ y = y ∧ x                    commutativity
3. x ∧ (y ∧ z) = (x ∧ y) ∧ z        associativity
4. A _top_ element, denoted ⊤, such that
        for all x in V, ⊤ ∧ x = x   identity?
5. Optionally, a _bottom_ element, denoted ⊥, such that:
        for all x in V, ⊥ ∧ x = ⊥   absorption?

A picture is worth 1000 words. Just look at 9.22 and the paragraph below for an intuition on all of this mumbo jumbo.

0 1 2
{} 0 1 2 {0,1} {0,2} {1,2} {0,1,2}

The family of transfer functions F: V -> V has the following properties:
1. F has an identity function I, such that I(x) = x for all x in V
2. F is closed under composition, that is for any two functions f and g in F, the function h is defined by h(x) = g ∘ f

The Iterative Algorithm for General Frameworks

In:
    Data-Flow Framework with the following components
    1. A Data-Flow Graph, with ENTRY and EXIT nodes
    2. A direction for the data-flow, D
    3. A set of values V (where a value is a set in situations like reachability analysis and variable liveness)
    4. A meet operator ∧
    5. A set of functions F, where f in F is the transfer function for a block, B
    6. A constnat value entry or exit in V, representing the boundary condition for forward and backward frameworks, respectively

Out:
    Values in V for IN[B] and OUT[B] for each block B in the data-flow graph

Method:
    First, note that the meet operator and transfer functions can be implemented as parameters to higher order functions,
    and so the iterative traversal of the graph can be shared between data-flow analysis problems.

    D is Forward:
        OUT[ENTRY] = entry value
        for each basic block B other than ENTRY: OUT[B] = ⊤
        while changes to any OUT occur:
            for each basic block B other than ENTRY:
                IN[B] = ∧ for each P such that P is a predecessor of B: OUT[P]
                OUT[B] = f(IN[B])

    D is Backward:
        IN[EXIT] = exit value
        for each basic block B other than EXIT: IN[B] = ⊤
        while changes to any IN occur:
            for each basic block B other than EXIT:
                OUT[B] = ∧ for each S such that S is a successor of B: IN[S]
                IN[B] = f(OUT[B])

See the breakdown near the end of "The Maximum Fixedpoint Versus the MOP Solution"

Constant Propagation
- has an unbounded set of possible data-flow values
- is not distributive

A single variable is in one of three states: Associated with a list of all constants appropriate for it's type, not-a-constant, or undefined

Data-flow values for the constant propagation framework are maps of each variable in the program to one of the values in the constant semilattice

The transfer function from map S to map T for a statement, s, is as follows:
T = f(S) = {
    S,                      if s is not an assignment statement
    S(v) = T(v)             if s is an assignment statement to variable x and x != v
    S(x) = c                if the RHS of statement s is a constant c
    S(x) = S(y) + S(z)      if the RHS has the form y + z and S(y) and S(z) are constants
        NAC                     if either S(y) or S(z) is NAC
        UNDEF                   otherwise
    NAC                     if the RHS is any other expression
}

Partial Redundancy Elimination

- used to minimize the number of expression evaluations
- the number of statements that evaluate x + y may be increased, but the goal is that for any given
    path the total number of evaluations of x + y is reduced
- this improves performance, because an operation is never applied unless it has to be
- requires 4 different kinds of data flow analysis

Redundancy exists in several forms:
- global common subexpressions
- loop invariant expressions
- partially redundant expressions

***
Note on loop invariant expressions:
if none of the expression's dependencies change within a loop, we can move it out of the loop
    UNLESS it's possible that the expression would not have been evaluated, so in the case where a loop condition
    isn't met. If the program would've never entered the loop, it would change the meaning of the program
    to execute the inner expression outside of the loop

    - the instruction throwing an exception is an exception that would not have arisen in the original program
    - when the loop exits early the newly "optimized" program would take more time - hardly an optimization
***

We can't eliminate all redundancy unless the compiler has the ability to add new blocks at-will

A _critical edge_ is an edge from a block with two or more successors to a block with two or more predecessors

The Lazy Code Motion Problem

Programs optimized to remove partial redundancies should result in the following:
- redundant computations that can be eliminated without duplicating code are eliminated
- the optimized program does not perform any computation that is not in the original program execution
- expressions are computed at the latest possible time

The goal is to compute a value as late as possible to minimize it's lifetime, and the amount of time it spends occupying a register

Full Redundancy:
An expression e in block B is redundant if along all paths reaching B, e has been evaluated and the operands of e
haven not been subsequently redefined.

Partial Redundancy:
The goal is to render partially redundant expressions fully redundant by copying them to other blocks in the flow graph

Anticipation of Expressions
- Copies of an expression must only be placed at points where the expression is _anticipated_.
- An expression is _anticipated_ at point p if all paths leading from the point p eventually compute the value of the expression b + c
    from the values b and c that are available at point p

Lazy Code Motion Algorithm

Almost all data flow analysis can be placed into one of four groups:
forwards, backwards, union, intersection

Loops in Flow Graphs

Programs spend most of their time executing loops, and so optimizing them can have a significant impact.

Node d in a flow graph _dominates_ node n, written d dom n, if every path from the entry node of the flow
    graph to n goes through d

A dominator tree can be used to represent this relationship in the flow graph
- nodes in the tree _dominate_ their descendants

A _natural loop_:
- must have a single entry node, called the _header_, which dominates all nodes in the loop
- A back edge must enter the loop header, i.e. form the loop
- this definition gives us the notion of an _innermost_ loop

Speed of Convergence of Iterative Data-Flow Algorithms
- the maximum number of iterations is the product of the height of the lattice and the number of nodes in the flow graph
- for many data-flow analysis it is possible to order the evaluation such that the algorithm
    converges in a much smaller number of iterations
- if useful information propagates along all acyclic paths, then we have the opportunity to tailor the order in which
    we visit nodes in the algorithms

Region-Based Analysis
- instead of creating transfer functions for blocks, region based analysis seeks to build up transfer functions for larger and large regions of the program
- ultimately we arrive data-flow values for entire procedures
- requires a semilattice of data-flow values and a semilattice of transfer functions that possess a meet operator, a composition operator, and a closure operator
- a program is viewed as a hierarchy of regions, i.e. portions of the flow graph with only one entry

Symbolic Analysis
- _affine_ expressions of _reference variables_
- an expression is affine with respect to v1,v2,...,vn if it can be expressed as c0 + c1v1 + ... + cnvn, where c0,c1,...,cn are constants
- "linear" expressions
- if we determine that the locations a loop writes to are different on every iteration, than the assignments may be candidates for parallelization

