Global optimizations are usually based on *data flow analysis*
- they usually have the same form: for each instruction, some property that must host each time the instruction is executed

A few different types of data-flow analyses:

* constant propagation analysis
    - at each point in the program and for each variable in use, does that variable hold a unique constant value
    - we can use this information to replace variable references (with might involve loading the value from memory into a register) with constant values
* liveness analysis
    - at each point in the program, is the value held by a particular variable sure to be overwritten before it's read
    - we then have no need to preserve that value in a register or in memory
* hierarchical analysis
    - used to eliminate "induction" variables (variables that count the number of iterations around a loop)

It's all about gathering information globally about the program.

***A compiler MUST preserve the semantics of the original program.***

In general, a compiler cannot understand enough context about a programmer's algorithm and it's intent
to replace it with a substantially different or better algorithm. We're confined to low-level transformations
involving algebraic identities or semantic properties.

Redundancy elimination can and should be opaque to the programmer

A compiler can improve a program without changing what it computes by:
* common-subexpression elimination
    - if none of the values E depends on change between E's computation and the next time E is encountered, we can just re-use E.
* copy-propagation
    - u = v is a _copy statement_
    - by replacing subsequent uses of u in the block with v, we set the stage for removing the copy instruction altogether
* dead-code elimination
    - a variable is _live_ if it's value can be used subsequently, otherwise it's _dead_
    - _dead code_ is statements that computed a value that is never used
    - after applying copy propagation and constant folding we might arrive in a situation where a block
        is wrapped in an if (false) { ... which means the entire block is dead code and thus eliminatable.
* constant folding
    - deducing that the value of an expression is constant, and replacing the expression with the constant

These are _semantics preserving_ transformations

Loops are important for optimization. Moving code out of the inner loop, even if it bloats the outer code, will reduce the running time of the algorithm

Code motion:
    while (i <= limit - 1)      <-- limit - 1 executed many times
vs
    l = limit - 1               <-- limit - 1 executed once
    while (i <= l)

Induction Variables
Strength Reduction

Data Flow Analysis
- A body of techniques that derive information about the flow of data along program execution paths
- the execution of a program can be viewed as a sequence of transformations on the program _state_
- Each instruction results in a transformation from State -> State
- So we build up all of the possible sequences of states, and then extract the data we need from the states to perform the transformations

An _execution path_ from point (state) p1 to point pn to be a sequence of points such that for each
i = 1, 2, ..., n - 1 either:
1. p(i) is the point immediately preceding a statement, s, and p(i + 1) is the point immediately following s
2. p(i) is the end of a block, and p(i + 1) is the beginning of a successor block

In general, there's an infinite number of execution paths through a program and no defined upper bound on the length of a particular execution path.
Also in general, it is not possible to keep track of all of the program states for all possible paths.
- so we keep track of only what must be kept track of to perform the analysis that we're trying to do.

Definitions that _may_ reach a program point along some path are known as _reaching_ definitions.

Denotation for data-flow values before and after statements:
IN[s], OUT[s]

*** The data-flow _problem_ is to find a solution to a set of constraints on the IN[s] and OUT[s] for all statements s.***
Two sets of constraints:
1. Semantics of the statements ("transfer functions")
2. Flow of control

Data flow values before and after a statement are constrained by the semantics of the statement
The relation between state s and state t, f(s) = t is known as the _transfer function_
- We can also define the inverse transfer function g(t) = s

Within a basic block, control-flow constraints:
IN[s(i + 1)] = OUT[s(i)]
Between basic blocks:
If we're interested in collecting the definitions that may reach a program point,
    then the set of definitions reaching the leader statement of a basic block is the union of the definitions
    after the last statements of each of the predecessor blocks.

What goes on inside of a basic block is quite simple, so we can save time and space by defining the data-flow
    schema in terms of data values entering and leaving basic blocks

The transfer function for an entire block is the composition of the transfer functions of the statements the block contains:
f(B) = f(sn) . f(s(n - 1)) . ... . f(s2) . f(s1)

Thus, f(IN[B]) = OUT[B]

Reaching Definitions
A definition d _reaches_ p if there is a path from the point immediately following d to p,
    such that d is not _killed_ along the path.
A definition d is _killed_ if it is redefined along the path.
A _definition_ of x is a statement that assigns, or may assign, a value to x

Program analysis must be conservative: If we don't know whether or not a statement is assigning a value to the variable x, we must ASSUME that it does.

One trick for detecting variables used before they're defined:
- introduce dummy definitions for all variables that have been used.
- if one of those dummy definitions reaches a point where the variable is used without being
    killed by the real definition, then we've probably got a use-before-definition situation on our hands

To decide whether each path in a flow graph can be taken is undecidable
So we must assume that every path can be taken.

It is always unacceptable for an optimization to change what the program computes, and so optimizations must be conservative.

Constraints for the Reaching Definitions Problem:

d: u = v + w "generates" a definition of u and "kills" other definitions of u in the program

So the transfer function for this definition is:

f(x) = gen U (x - kill) where gen is { d } and kill is all of the other definitions of u in the program

This can be defined for statements by using the composition rule above.

Like a statement, a basic block also generates a set of definitions and kills a set of definitions.

Constraints for Control-Flow

The "meet" operator for reaching definitions is the confluence of different paths merging together in the flow graph.

***
The reaching definitions problem equations are then:
OUT[ENTRY] = {}
OUT[B] = gen(B) U (IN[B] - KILL(B))
IN[B] = U(P is a predecessor of B) OUT[P]
***

The Algorithm for Reaching Definitions
In:
    Flow graph for which the kill set and the gen set have been computed for each block
Out:
    IN[B] and OUT[B] the set of definitions "reaching" the entry and exit of each block

Least fixed-point. Start with OUT[B] = {} for all blocks, and then converge on the desired values

Until the set of OUT[B] for each block does not change:

OUT[ENTRY] = {}
for each basic block B other than ENTRY: OUT[B] = {}
while changes to the set of OUT[B] sets occur
    for each basic block B other than ENTRY
        IN[B] = UNION(each predecessor P of B: OUT[P])
        OUT[B] = genB U (IN[B] - killB)

If the basic blocks are properly ordered for the inner for loop, empirical evidence shows that the while loop will only iterate at most 5 times.

The book represents representing definition sets as bit vectors (and I agree).

Live-Variable Analysis

We wish to know whether the value of variable x at point p could be used along any of the paths leading from point p
If it can be used, we say the variable is _live_
Otherwise, it's _dead_

In:
    Flow graph with def and use computed for each block
Out:
    IN[B] and OUT[B] the set of variables live on entry and exit of each block B in the flow graph

IN[EXIT] = {}
for each basic block B other than EXIT: IN[B] = {}
while changes to the set of IN[B] sets occur
    for each basic block B other than EXIT:
        OUT[B] = UNION(each succcessor S of B: IN[S])
        IN[B] = useB U (OUT[B] - defB)

Available Expressions

Used for finding common subexpressions across blocks. Similar to the above.

Each data-flow problem is defined by:
1. The domain of the data-flow values
2. The direction of the data-flow
3. The family of transfer functions
4. The boundary condition
5. The meet operator

Figure 9.21 summarizes each for the reaching definitions, live variables, and availabe expression data-flow analysis problems

Data Flow Analysis Framework

A 4-tuple: (D, V, ∧, F)
1. the direction of the data flow, D
2. A semilattice consisting of the domain of values V and the meet operator ∧
3. Family F of transfer functions from V to V
    - must include functions suitable for boundary conditions, i.e. constant transfer functions

A *Semilattice* is a set V with a binary operator ∧ such that for all x, y, and z in V:
1. x ∧ x = x                        idempotency
2. x ∧ y = y ∧ x                    commutativity
3. x ∧ (y ∧ z) = (x ∧ y) ∧ z        associativity
4. A _top_ element, denoted ⊤, such that
        for all x in V, ⊤ ∧ x = x   identity?
5. Optionally, a _bottom_ element, denoted ⊥, such that:
        for all x in V, ⊥ ∧ x = ⊥   absorption?

A picture is worth 1000 words. Just look at 9.22 and the paragraph below for an intuition on all of this mumbo jumbo.

0 1 2
{} 0 1 2 {0,1} {0,2} {1,2} {0,1,2}

The family of transfer functions F: V -> V has the following properties:
1. F has an identity function I, such that I(x) = x for all x in V
2. F is closed under composition, that is for any two functions f and g in F, the function h is defined by h(x) = g ∘ f

The Iterative Algorithm for General Frameworks

In:
    Data-Flow Framework with the following components
    1. A Data-Flow Graph, with ENTRY and EXIT nodes
    2. A direction for the data-flow, D
    3. A set of values V (where a value is a set in situations like reachability analysis and variable liveness)
    4. A meet operator ∧
    5. A set of functions F, where f in F is the transfer function for a block, B
    6. A constnat value entry or exit in V, representing the boundary condition for forward and backward frameworks, respectively

Out:
    Values in V for IN[B] and OUT[B] for each block B in the data-flow graph

Method:
    First, note that the meet operator and transfer functions can be implemented as parameters to higher order functions,
    and so the iterative traversal of the graph can be shared between data-flow analysis problems.

    D is Forward:
        OUT[ENTRY] = entry value
        for each basic block B other than ENTRY: OUT[B] = ⊤
        while changes to any OUT occur:
            for each basic block B other than ENTRY:
                IN[B] = ∧ for each P such that P is a predecessor of B: OUT[P]
                OUT[B] = f(IN[B])

    D is Backward:
        IN[EXIT] = exit value
        for each basic block B other than EXIT: IN[B] = ⊤
        while changes to any IN occur:
            for each basic block B other than EXIT:
                OUT[B] = ∧ for each S such that S is a successor of B: IN[S]
                IN[B] = f(OUT[B])

See the breakdown near the end of "The Maximum Fixedpoint Versus the MOP Solution"



